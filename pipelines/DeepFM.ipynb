{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import random as rn\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data input pipeline\n",
    "\n",
    "Data refers to the kaggle competition (Porto seguro Safe Driver Prediction).\n",
    "\n",
    "The data input pipelines achieves the goals:\n",
    "\n",
    "1. Re-encode the id for values of each column for the convenience of feteching embeddings. \n",
    "2. Transform the format of data from standard csv format to \"{feat_id: feat_value}\" format, as shown in write_to_csv function.\n",
    "3. Distinguish categorical columns from numerical columns. Deal with them in different ways.\n",
    "4. Split training data into two partitions: training data and validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ori_data(root_dir=\"../data/porto-seguro-safe-driver-prediction\", train_fn=\"train.csv\", test_fn=\"test.csv\"):\n",
    "    train_fn = os.path.join(root_dir, train_fn)\n",
    "    test_fn = os.path.join(root_dir, test_fn)\n",
    "    \n",
    "    train = pd.read_csv(train_fn, index_col='id')\n",
    "    test = pd.read_csv(test_fn, index_col='id')\n",
    "    \n",
    "    train_label = train[\"target\"]\n",
    "    train.drop(\"target\", axis=1, inplace=True)\n",
    "    \n",
    "    return train, train_label, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cols_by_type(data, threshold):\n",
    "    '''\n",
    "    Distinguish the categorical columns from the numerical columns based on the number of unique values.\n",
    "    '''\n",
    "    num_cols = []\n",
    "    cat_cols = []\n",
    "    for col in data.columns:\n",
    "        if data[col].unique().shape[0] > threshold:\n",
    "            num_cols.append(col)\n",
    "        else:\n",
    "            cat_cols.append(col)\n",
    "    return np.array(num_cols), np.array(cat_cols)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(fn, data, field_size, feat_size, label=None, label_name=\"\"):   \n",
    "    '''\n",
    "    The format is as follows:\n",
    "    The first line includes two integer. They are respectively field_size andd feat_size\n",
    "    The following contains data which is in the format of \"feat_index: feat_value\"\n",
    "    '''\n",
    "    with open(fn, 'w') as f:\n",
    "        f.write(\"{} {}\\n\".format(field_size, feat_size))\n",
    "        label_name_str = \" {}\".format(label_name) if label_name else \"\"\n",
    "        f.write(data.index.name + \" \" + \" \".join(data.columns) + label_name_str + \"\\n\")\n",
    "        indices = data.index\n",
    "        for row_idx, row in enumerate(data.values):\n",
    "            label_val_str = \" \" + str(label.iloc[row_idx]) if label_name else \"\"     \n",
    "            \n",
    "            line =  str(indices[row_idx]) + \" \" + \" \".join([\"{}:{}\".format(val[0], val[1]) for val in row]) + label_val_str + \"\\n\"\n",
    "            f.write(line)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mapping_tabel(fn, data):\n",
    "    with open(fn, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(root_dir=\"../data/porto-seguro-safe-driver-prediction\", train_ds_fn=\"tmp/train_ds.csv\", test_ds_fn=\"tmp/test_ds.csv\"):\n",
    "    train_ds_fn = os.path.join(root_dir, train_ds_fn)\n",
    "    test_ds_fn = os.path.join(root_dir, test_ds_fn)\n",
    "    \n",
    "    print(\"reading raw data...\")\n",
    "    train, train_label, test = read_ori_data(root_dir)\n",
    "    \n",
    "    train_test = pd.concat([train, test], axis=0)    \n",
    "    \n",
    "    field_size = train_test.shape[1]\n",
    "    feat_size = 0\n",
    "    featidx2onehot = defaultdict(dict)\n",
    "    onehot2featidx = defaultdict(dict)\n",
    "\n",
    "    num_cols, cat_cols = split_cols_by_type(train_test, 100)\n",
    "    \n",
    "    print(\"encoding data...\")\n",
    "    for col_idx, column in enumerate(train_test.columns):    \n",
    "        start = time.time()\n",
    "        if column in cat_cols:\n",
    "            temp = {}\n",
    "            # create mapping table         \n",
    "            col = train_test[column].unique()\n",
    "            col.sort()\n",
    "            for idx in col:\n",
    "                temp[idx] = feat_size\n",
    "                featidx2onehot[column][feat_size] = idx.item()\n",
    "                onehot2featidx[column][idx.item()] = feat_size\n",
    "                feat_size += 1        \n",
    "\n",
    "            # encode data\n",
    "            train_test[column] = train_test[column].map(lambda x: (temp[x], 1))\n",
    "        else:\n",
    "            featidx2onehot[column][feat_size] = 0\n",
    "            onehot2featidx[column][0] = feat_size\n",
    "            feat_size += 1\n",
    "\n",
    "            train_test[column] = train_test[column].map(lambda x: (feat_size-1, x))        \n",
    "\n",
    "        print(\"column: {} time: {}\".format(column, time.time() - start))    \n",
    "    \n",
    "    processed_train = train_test.loc[train.index]\n",
    "    processed_test = train_test.loc[test.index]\n",
    "    \n",
    "    print(\"Write data to files...\")\n",
    "    write_to_csv(train_ds_fn, processed_train, field_size, feat_size, train_label, \"target\")\n",
    "    write_to_csv(test_ds_fn, processed_test, field_size, feat_size)    \n",
    "    \n",
    "    featidx2onehot_fn = os.path.join(root_dir, \"tmp/featidx_to_onehot.csv\")\n",
    "    onehot2featidx_fn = os.path.join(root_dir, \"tmp/onehot_to_featidx.csv\")\n",
    "    \n",
    "    write_mapping_tabel(featidx2onehot_fn, dict(featidx2onehot))\n",
    "    write_mapping_tabel(onehot2featidx_fn, dict(onehot2featidx))\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(is_training):\n",
    "    \n",
    "    def parse(line):\n",
    "        features = tf.strings.split(line)\n",
    "        if is_training:\n",
    "            label = features[-1]\n",
    "            label = tf.strings.to_number(label, out_type=tf.int32)        \n",
    "            features = tf.strings.split(features[1:-1], \":\")\n",
    "        else:\n",
    "            features = tf.strings.split(features[1:], \":\")            \n",
    "        feat_id = features[:, 0:1].values\n",
    "        feat_values = features[:, 1:2].values\n",
    "        \n",
    "        feat_id = tf.strings.to_number(feat_id, out_type=tf.int32)\n",
    "        feat_values = tf.strings.to_number(feat_values, out_type=tf.float32)        \n",
    "        \n",
    "        if is_training:\n",
    "            res = {\"feat_id\": feat_id, \"feat_values\": feat_values, \"label\": label}\n",
    "        else:\n",
    "            res = {\"feat_id\": feat_id, \"feat_values\": feat_values}\n",
    "        return res\n",
    "    \n",
    "    return parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_by_dataset(root_dir=\"../data/porto-seguro-safe-driver-prediction\", fn=\"tmp/train_ds.csv\", is_training=True):\n",
    "    '''\n",
    "    Reading data by the TextLineDataset datastructure.\n",
    "    \n",
    "    is_training: is_training being true denotes that the data contains label column\n",
    "    '''\n",
    "    ds_fn = os.path.join(root_dir, fn)\n",
    "    \n",
    "    with open(ds_fn, 'r') as f:\n",
    "        field_size, feat_size = list(map(int, f.readline().strip().split()))\n",
    "        f.close()\n",
    "    \n",
    "    data = tf.data.TextLineDataset(ds_fn).skip(2)    \n",
    "    data = data.map(parse_line(True), num_parallel_calls=4)\n",
    "    \n",
    "    return field_size, feat_size, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_by_pd(root_dir=\"../data/porto-seguro-safe-driver-prediction\", fn=\"tmp/train_ds.csv\", is_training=True):\n",
    "    '''\n",
    "    Readd data by pandas\n",
    "    '''\n",
    "    ds_fn = os.path.join(root_dir, fn)\n",
    "\n",
    "    with open(ds_fn, 'r') as f:\n",
    "        field_size, feat_size = list(map(int, f.readline().strip().split()))\n",
    "        f.close()    \n",
    "        \n",
    "    data = pd.read_csv(ds_fn, skiprows=1, sep=\" \", index_col='id')  \n",
    "#     print(data.head(1))\n",
    "    if is_training:\n",
    "        label = data['target']\n",
    "        data = data.drop(['target'], axis=1)\n",
    "        res = {\"feat_id\": data.copy(), \"feat_values\": data.copy(), \"label\": label}\n",
    "        \n",
    "        res[\"label\"] = tf.convert_to_tensor(res[\"label\"].values, dtype=tf.int32)            \n",
    "    else:\n",
    "        res = {\"feat_id\": data.copy(), \"feat_values\": data.copy()}\n",
    "\n",
    "    for col in data.columns:\n",
    "        res[\"feat_id\"][col] = data[col].map(lambda x: x.split(\":\")[0])\n",
    "        res[\"feat_values\"][col] = data[col].map(lambda x: x.split(\":\")[1])\n",
    "        \n",
    "    res[\"feat_id\"] = tf.convert_to_tensor(res[\"feat_id\"].values, dtype=tf.int32)\n",
    "    res[\"feat_values\"] = tf.convert_to_tensor(res[\"feat_values\"].values, dtype=tf.float32)\n",
    "\n",
    "    \n",
    "    return field_size, feat_size, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_stratification(root_dir=\"../data/porto-seguro-safe-driver-prediction\", fn=\"tmp/train_ds.csv\", train_fn=\"tmp/train_train.csv\", val_fn=\"tmp/train_val.csv\"):\n",
    "    input_fn = os.path.join(root_dir, fn)\n",
    "    train_fn = os.path.join(root_dir, train_fn)\n",
    "    val_fn = os.path.join(root_dir, val_fn)\n",
    "    \n",
    "    with open(input_fn, 'r') as f:\n",
    "        line = f.readline()\n",
    "        f.close()\n",
    "    \n",
    "    data = pd.read_csv(input_fn, skiprows=1, sep=\" \", index_col='id')    \n",
    "    train, validation = train_test_split(data, test_size=0.1, random_state=200, stratify=data['target'])\n",
    "    \n",
    "    def write_to(fn, data, first_lines):\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(first_lines)\n",
    "            f.close()\n",
    "            \n",
    "        data.to_csv(fn, sep=\" \", mode='a')\n",
    "    \n",
    "    write_to(train_fn, train, line)\n",
    "    write_to(val_fn, validation, line)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading raw data...\n",
      "encoding data...\n",
      "column: ps_ind_01 time: 0.8766317367553711\n",
      "column: ps_ind_02_cat time: 0.746898889541626\n",
      "column: ps_ind_03 time: 0.719944953918457\n",
      "column: ps_ind_04_cat time: 0.7144441604614258\n",
      "column: ps_ind_05_cat time: 0.6973628997802734\n",
      "column: ps_ind_06_bin time: 0.6936299800872803\n",
      "column: ps_ind_07_bin time: 0.7055909633636475\n",
      "column: ps_ind_08_bin time: 0.6214621067047119\n",
      "column: ps_ind_09_bin time: 1.1251420974731445\n",
      "column: ps_ind_10_bin time: 0.9410827159881592\n",
      "column: ps_ind_11_bin time: 0.794482946395874\n",
      "column: ps_ind_12_bin time: 1.069087028503418\n",
      "column: ps_ind_13_bin time: 0.8360259532928467\n",
      "column: ps_ind_14 time: 0.7426760196685791\n",
      "column: ps_ind_15 time: 0.6626708507537842\n",
      "column: ps_ind_16_bin time: 0.601754903793335\n",
      "column: ps_ind_17_bin time: 0.5626480579376221\n",
      "column: ps_ind_18_bin time: 0.6173157691955566\n",
      "column: ps_reg_01 time: 0.5989120006561279\n",
      "column: ps_reg_02 time: 0.5386519432067871\n",
      "column: ps_reg_03 time: 0.4512488842010498\n",
      "column: ps_car_01_cat time: 0.6837091445922852\n",
      "column: ps_car_02_cat time: 0.673224925994873\n",
      "column: ps_car_03_cat time: 0.6494307518005371\n",
      "column: ps_car_04_cat time: 0.6892151832580566\n",
      "column: ps_car_05_cat time: 0.6268720626831055\n",
      "column: ps_car_06_cat time: 0.571774959564209\n",
      "column: ps_car_07_cat time: 0.6746652126312256\n",
      "column: ps_car_08_cat time: 0.7647972106933594\n",
      "column: ps_car_09_cat time: 0.8555400371551514\n",
      "column: ps_car_10_cat time: 0.7593257427215576\n",
      "column: ps_car_11_cat time: 0.48775696754455566\n",
      "column: ps_car_11 time: 0.7285540103912354\n",
      "column: ps_car_12 time: 0.5259890556335449\n",
      "column: ps_car_13 time: 0.46744513511657715\n",
      "column: ps_car_14 time: 0.502194881439209\n",
      "column: ps_car_15 time: 0.9797532558441162\n",
      "column: ps_calc_01 time: 0.8332562446594238\n",
      "column: ps_calc_02 time: 0.730679988861084\n",
      "column: ps_calc_03 time: 0.8304016590118408\n",
      "column: ps_calc_04 time: 1.1405630111694336\n",
      "column: ps_calc_05 time: 0.6915180683135986\n",
      "column: ps_calc_06 time: 0.6644489765167236\n",
      "column: ps_calc_07 time: 0.6840560436248779\n",
      "column: ps_calc_08 time: 0.6931099891662598\n",
      "column: ps_calc_09 time: 0.7008929252624512\n",
      "column: ps_calc_10 time: 0.6435461044311523\n",
      "column: ps_calc_11 time: 0.5919859409332275\n",
      "column: ps_calc_12 time: 0.604640007019043\n",
      "column: ps_calc_13 time: 0.5866222381591797\n",
      "column: ps_calc_14 time: 0.5776388645172119\n",
      "column: ps_calc_15_bin time: 0.6309540271759033\n",
      "column: ps_calc_16_bin time: 0.6078729629516602\n",
      "column: ps_calc_17_bin time: 0.5896308422088623\n",
      "column: ps_calc_18_bin time: 0.583935022354126\n",
      "column: ps_calc_19_bin time: 0.568742036819458\n",
      "column: ps_calc_20_bin time: 0.5906600952148438\n",
      "Write data to files...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "encode_data()\n",
    "train_test_stratification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The original data is imblanced. The one of solutions is to downsample the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample(root_dir, fn, output_fn):\n",
    "    input_fn = os.path.join(root_dir, fn)\n",
    "    output_fn = os.path.join(root_dir, output_fn)\n",
    "    \n",
    "    with open(input_fn, 'r') as f:\n",
    "        line = f.readline()\n",
    "        f.close()\n",
    "    \n",
    "    data = pd.read_csv(input_fn, skiprows=1, sep=\" \", index_col='id')    \n",
    "    \n",
    "    positive_data = data[data['target'] == 1]\n",
    "    negative_data = data[data['target'] == 0]\n",
    "    negative_data = negative_data.sample(n = positive_data.shape[0])\n",
    "    data = pd.concat([positive_data, negative_data], axis=0)\n",
    "    \n",
    "    def write_to(fn, data, first_lines):\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(first_lines)\n",
    "            f.close()\n",
    "            \n",
    "        data.to_csv(fn, sep=\" \", mode='a')\n",
    "    \n",
    "    write_to(output_fn, data, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down_sample('../data/porto-seguro-safe-driver-prediction', 'tmp/train_train.csv', 'tmp/train_train_balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Training data is read through the TextlineDataset proveded by Tensorflow. It takes the advantages of efficiently processing data and automatically generating batch data. Validation and testing data is read by pandas with the goal of readily programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_size, feat_size, train = read_by_dataset(fn=\"tmp/train_train_balanced.csv\", is_training=True)\n",
    "_, _, validation = read_by_pd(fn=\"tmp/train_val.csv\", is_training=True)\n",
    "_, _,  test = read_by_pd(fn=\"tmp/test_ds.csv\", is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepPart(keras.layers.Layer):\n",
    "    def __init__(self, deep_hidden_units, deep_drop_probs):\n",
    "        super(DeepPart, self).__init__()\n",
    "        self.dense_layers = [keras.layers.Dense(hidden_units, activation='relu') for hidden_units in deep_hidden_units]\n",
    "        self.dropout_layers = [keras.layers.Dropout(dropout_prob) for dropout_prob in deep_drop_probs]\n",
    "        self.bn_layers = [keras.layers.BatchNormalization() for _ in range(3)]\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        outputs = inputs\n",
    "        for dense_layer, dropout_layer, bn_layer in zip(self.dense_layers, self.dropout_layers, self.bn_layers):\n",
    "            outputs = dense_layer(outputs)\n",
    "            outputs = bn_layer(outputs, training=training)\n",
    "            outputs = dropout_layer(outputs, training=training)\n",
    "        return outputs\n",
    "\n",
    "class DeepFM(keras.Model):\n",
    "    \n",
    "    def __init__(self, field_size, feat_size, embedding_dim, deep_hidden_units, deep_drop_probs):\n",
    "        super(DeepFM, self).__init__()\n",
    "        \n",
    "        self.field_size = field_size\n",
    "        self.feat_size = feat_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.one_order_embed = keras.layers.Embedding(self.feat_size, 1)\n",
    "        self.embedding = keras.layers.Embedding(self.feat_size, embedding_dim)\n",
    "        \n",
    "        self.deep = DeepPart(deep_hidden_units, deep_drop_probs)\n",
    "        \n",
    "        self.output_dense = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, feat_idx, feat_values, training):\n",
    "        # feat_id: [None, field_size]\n",
    "        # feat_values: [None, field_size]\n",
    "        \n",
    "        # FM one-order part        \n",
    "        one_order_weights = self.one_order_embed(feat_idx) # [None, field_size, 1]\n",
    "        one_order_weights = tf.reshape(one_order_weights, shape=(one_order_weights.shape[0], -1)) # [None, field_size]\n",
    "        weighted_values = tf.multiply(one_order_weights, feat_values) # [None, field_size]\n",
    "        sum_weighted_values = tf.reduce_sum(weighted_values, axis=1) # [None, ]\n",
    "        \n",
    "        # FM second-order part\n",
    "        embeddings = self.embedding(feat_idx) # [None, field_size, embedding_dim]\n",
    "        high_dim_feat_values = tf.reshape(feat_values, shape=(feat_values.shape[0], feat_values.shape[1], 1)) # [None, field_size, 1]\n",
    "        squared_of_sum = tf.square(tf.reduce_sum(tf.multiply(embeddings, high_dim_feat_values), axis=1)) # [None, embedding_dim]\n",
    "        sum_of_squared = tf.reduce_sum(tf.square(tf.multiply(embeddings, high_dim_feat_values)), axis=1) # [None, embedding_dim]        \n",
    "        second_order_weighted_valaues = 0.5 * tf.reduce_sum(squared_of_sum - sum_of_squared, axis=1) # [None, ]\n",
    "\n",
    "        # Deep part\n",
    "        cat_embeddings = tf.reshape(embeddings, shape=(-1, self.field_size * self.embedding_dim)) # [None, self.field_size * self.embedding_dim]        \n",
    "        deep_weighted_values = tf.reshape(self.deep(cat_embeddings, training), [-1]) # [None, 1]\n",
    "        \n",
    "        out = self.output_dense(tf.stack([sum_weighted_values, second_order_weighted_valaues, deep_weighted_values], axis=1)) # [None, 3] -> [None, 1]\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class DeepFMWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def __loss_function(self, real, prediction):\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real, prediction), axis=0)\n",
    "        # Deal with imbalanced Data        \n",
    "        # return tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(real, prediction, 10), axis=0)\n",
    "\n",
    "    def __evaluate(self, real, prediction):\n",
    "        prediction = tf.nn.sigmoid(prediction)\n",
    "\n",
    "        metric = keras.metrics.AUC()        \n",
    "        auc = metric(real, prediction)\n",
    "        \n",
    "        round_pred = tf.round(prediction)\n",
    "        \n",
    "        metric_recall = keras.metrics.Recall()\n",
    "        metric_precision = keras.metrics.Precision()\n",
    "        recall = metric_recall(real, round_pred)\n",
    "        precision = metric_precision(real, round_pred)\n",
    "        \n",
    "        res = {\"auc\": auc.numpy(), \"recall\": recall.numpy(), \"precision\": precision.numpy()}\n",
    "        return res\n",
    "    \n",
    "    def __run_on_single_batch(self, batch_data, is_training=False, optimizer=None):\n",
    "        batch_label = tf.cast(tf.reshape(batch_data['label'], shape=[-1, 1]), dtype=tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self.model(batch_data['feat_id'], batch_data['feat_values'], training=is_training)\n",
    "            loss = self.__loss_function(batch_label, prediction)\n",
    "            evaluation = self.__evaluate(batch_label, prediction)        \n",
    "            \n",
    "        if is_training:\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))            \n",
    "            \n",
    "        return loss, evaluation\n",
    "    \n",
    "    def fit(self, train, validation, epochs=10, learning_rate=1e-4, batch_size=128, optimizer=None):\n",
    "        '''\n",
    "        train: dataset\n",
    "        validation: dic, {'feature_id': [], 'feature_values': [], 'label': []}\n",
    "        '''\n",
    "        start = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            for batch, batch_data in enumerate(train.shuffle(50000).batch(batch_size)):\n",
    "                loss, evaluation = self.__run_on_single_batch(batch_data, is_training=True, optimizer=optimizer)\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    end = time.time() - start\n",
    "                    print(\"Time: {}, Epoch: {}, Batch: {}, training loss: {}, training evaluation: {}\".format(end, epoch, batch, loss, evaluation))\n",
    "\n",
    "            val_loss, val_eval = self.__run_on_single_batch(validation)\n",
    "            print(\"Epoch: {}, validation loss: {}, validation evaluation: {}\".format(epoch, val_loss, val_eval))    \n",
    "    \n",
    "    def predict(self, test):\n",
    "        prediction = self.model(test['feat_id'], test['feat_values'], training=False)\n",
    "        return prediction    \n",
    "    \n",
    "    def save_model(self, fn):\n",
    "        self.model.save_weights(fn, save_format='tf')\n",
    "    \n",
    "    def load_model(self, fn):\n",
    "        self.model.load_weights(fn)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5.173868894577026, Epoch: 0, Batch: 0, training loss: [0.9974171], training evaluation: {'auc': 0.48882443, 'recall': 0.55725193, 'precision': 0.49324325}\n",
      "Time: 16.367104053497314, Epoch: 0, Batch: 100, training loss: [0.81736386], training evaluation: {'auc': 0.54886156, 'recall': 0.4651163, 'precision': 0.5555556}\n",
      "Epoch: 0, validation loss: [0.53288054], validation evaluation: {'auc': 0.56550205, 'recall': 0.20470263, 'precision': 0.049854033}\n",
      "Time: 35.46135187149048, Epoch: 1, Batch: 0, training loss: [0.78109705], training evaluation: {'auc': 0.5650735, 'recall': 0.45833334, 'precision': 0.52380955}\n",
      "Time: 48.46153116226196, Epoch: 1, Batch: 100, training loss: [0.7517545], training evaluation: {'auc': 0.53317446, 'recall': 0.5, 'precision': 0.48818898}\n",
      "Epoch: 1, validation loss: [0.5812465], validation evaluation: {'auc': 0.575358, 'recall': 0.23928078, 'precision': 0.049736463}\n",
      "Time: 66.0647759437561, Epoch: 2, Batch: 0, training loss: [0.73345333], training evaluation: {'auc': 0.5660622, 'recall': 0.50442475, 'precision': 0.4871795}\n",
      "Time: 81.38199281692505, Epoch: 2, Batch: 100, training loss: [0.7292876], training evaluation: {'auc': 0.5580175, 'recall': 0.5736434, 'precision': 0.54814816}\n",
      "Epoch: 2, validation loss: [0.6185995], validation evaluation: {'auc': 0.58622146, 'recall': 0.33748272, 'precision': 0.051658433}\n",
      "Time: 98.77865600585938, Epoch: 3, Batch: 0, training loss: [0.6781888], training evaluation: {'auc': 0.5860153, 'recall': 0.60305345, 'precision': 0.5486111}\n",
      "Time: 109.92304992675781, Epoch: 3, Batch: 100, training loss: [0.7503393], training evaluation: {'auc': 0.5283272, 'recall': 0.53846157, 'precision': 0.530303}\n",
      "Epoch: 3, validation loss: [0.6070937], validation evaluation: {'auc': 0.6021659, 'recall': 0.32319042, 'precision': 0.057681233}\n",
      "Time: 124.37106609344482, Epoch: 4, Batch: 0, training loss: [0.76827407], training evaluation: {'auc': 0.5757969, 'recall': 0.57042253, 'precision': 0.63779527}\n",
      "Time: 134.9341402053833, Epoch: 4, Batch: 100, training loss: [0.700762], training evaluation: {'auc': 0.63244563, 'recall': 0.61157024, 'precision': 0.5362319}\n",
      "Epoch: 4, validation loss: [0.6768485], validation evaluation: {'auc': 0.6096197, 'recall': 0.46611342, 'precision': 0.054171354}\n",
      "Time: 149.32924699783325, Epoch: 5, Batch: 0, training loss: [0.7236992], training evaluation: {'auc': 0.5757252, 'recall': 0.5496183, 'precision': 0.5625}\n",
      "Time: 160.01746702194214, Epoch: 5, Batch: 100, training loss: [0.7445483], training evaluation: {'auc': 0.59178627, 'recall': 0.5801527, 'precision': 0.61290324}\n",
      "Epoch: 5, validation loss: [0.66990036], validation evaluation: {'auc': 0.61005396, 'recall': 0.4449055, 'precision': 0.055606775}\n",
      "Time: 174.3574571609497, Epoch: 6, Batch: 0, training loss: [0.7198607], training evaluation: {'auc': 0.6359351, 'recall': 0.53333336, 'precision': 0.6605505}\n",
      "Time: 185.40157508850098, Epoch: 6, Batch: 100, training loss: [0.67313683], training evaluation: {'auc': 0.60621333, 'recall': 0.5322581, 'precision': 0.5641026}\n",
      "Epoch: 6, validation loss: [0.66386586], validation evaluation: {'auc': 0.6166817, 'recall': 0.43015215, 'precision': 0.05720417}\n",
      "Time: 200.40755915641785, Epoch: 7, Batch: 0, training loss: [0.6654938], training evaluation: {'auc': 0.61427784, 'recall': 0.47482014, 'precision': 0.6168224}\n",
      "Time: 212.83671402931213, Epoch: 7, Batch: 100, training loss: [0.6538055], training evaluation: {'auc': 0.67233586, 'recall': 0.49618322, 'precision': 0.64356434}\n",
      "Epoch: 7, validation loss: [0.6965077], validation evaluation: {'auc': 0.61764836, 'recall': 0.5039189, 'precision': 0.05441059}\n",
      "Time: 227.26137399673462, Epoch: 8, Batch: 0, training loss: [0.66569376], training evaluation: {'auc': 0.62442005, 'recall': 0.515873, 'precision': 0.5652174}\n",
      "Time: 238.583172082901, Epoch: 8, Batch: 100, training loss: [0.68733704], training evaluation: {'auc': 0.61534685, 'recall': 0.52554744, 'precision': 0.6}\n",
      "Epoch: 8, validation loss: [0.7603415], validation evaluation: {'auc': 0.6121312, 'recall': 0.63347167, 'precision': 0.04788124}\n",
      "Time: 253.2918040752411, Epoch: 9, Batch: 0, training loss: [0.68752897], training evaluation: {'auc': 0.63278955, 'recall': 0.6854839, 'precision': 0.56666666}\n",
      "Time: 264.590283870697, Epoch: 9, Batch: 100, training loss: [0.6811199], training evaluation: {'auc': 0.6293908, 'recall': 0.45762712, 'precision': 0.58064514}\n",
      "Epoch: 9, validation loss: [0.65356034], validation evaluation: {'auc': 0.61791366, 'recall': 0.43891194, 'precision': 0.056747735}\n",
      "Time: 280.49179005622864, Epoch: 10, Batch: 0, training loss: [0.66393363], training evaluation: {'auc': 0.6408123, 'recall': 0.5671642, 'precision': 0.66086954}\n",
      "Time: 292.031858921051, Epoch: 10, Batch: 100, training loss: [0.6512861], training evaluation: {'auc': 0.6528926, 'recall': 0.5123967, 'precision': 0.60784316}\n",
      "Epoch: 10, validation loss: [0.6309524], validation evaluation: {'auc': 0.61762947, 'recall': 0.41032735, 'precision': 0.057912547}\n",
      "Time: 306.7956521511078, Epoch: 11, Batch: 0, training loss: [0.6632613], training evaluation: {'auc': 0.7052755, 'recall': 0.5070422, 'precision': 0.75789475}\n",
      "Time: 318.6802599430084, Epoch: 11, Batch: 100, training loss: [0.67978746], training evaluation: {'auc': 0.6359045, 'recall': 0.58518517, 'precision': 0.6422764}\n",
      "Epoch: 11, validation loss: [0.6734126], validation evaluation: {'auc': 0.6200121, 'recall': 0.48086676, 'precision': 0.054767907}\n",
      "Time: 334.1990041732788, Epoch: 12, Batch: 0, training loss: [0.67680883], training evaluation: {'auc': 0.63209355, 'recall': 0.5840708, 'precision': 0.57391304}\n",
      "Time: 345.11671018600464, Epoch: 12, Batch: 100, training loss: [0.70645076], training evaluation: {'auc': 0.62204576, 'recall': 0.52671754, 'precision': 0.6448598}\n",
      "Epoch: 12, validation loss: [0.6859861], validation evaluation: {'auc': 0.6186861, 'recall': 0.5131397, 'precision': 0.052952092}\n",
      "Time: 359.59076595306396, Epoch: 13, Batch: 0, training loss: [0.6601953], training evaluation: {'auc': 0.63992035, 'recall': 0.5441176, 'precision': 0.62711865}\n",
      "Time: 370.8761520385742, Epoch: 13, Batch: 100, training loss: [0.6677939], training evaluation: {'auc': 0.602066, 'recall': 0.58156025, 'precision': 0.6074074}\n",
      "Epoch: 13, validation loss: [0.6843473], validation evaluation: {'auc': 0.61832595, 'recall': 0.5260489, 'precision': 0.05289509}\n",
      "Time: 385.4075791835785, Epoch: 14, Batch: 0, training loss: [0.6465666], training evaluation: {'auc': 0.6715806, 'recall': 0.6119403, 'precision': 0.6456693}\n",
      "Time: 396.4213778972626, Epoch: 14, Batch: 100, training loss: [0.668867], training evaluation: {'auc': 0.64616466, 'recall': 0.5298507, 'precision': 0.6339286}\n",
      "Epoch: 14, validation loss: [0.73623616], validation evaluation: {'auc': 0.6150182, 'recall': 0.63116646, 'precision': 0.049372476}\n",
      "Time: 410.8360381126404, Epoch: 15, Batch: 0, training loss: [0.6963556], training evaluation: {'auc': 0.61603856, 'recall': 0.6557377, 'precision': 0.5405405}\n",
      "Time: 421.95897793769836, Epoch: 15, Batch: 100, training loss: [0.6948304], training evaluation: {'auc': 0.66297746, 'recall': 0.6041667, 'precision': 0.70731705}\n",
      "Epoch: 15, validation loss: [0.6269308], validation evaluation: {'auc': 0.62004465, 'recall': 0.42876902, 'precision': 0.058601134}\n",
      "Time: 438.3564410209656, Epoch: 16, Batch: 0, training loss: [0.6486422], training evaluation: {'auc': 0.6523722, 'recall': 0.42962962, 'precision': 0.6666667}\n",
      "Time: 453.4886031150818, Epoch: 16, Batch: 100, training loss: [0.67215466], training evaluation: {'auc': 0.66746134, 'recall': 0.59398496, 'precision': 0.63709676}\n",
      "Epoch: 16, validation loss: [0.69947064], validation evaluation: {'auc': 0.61885566, 'recall': 0.56523746, 'precision': 0.051642798}\n",
      "Time: 469.83933687210083, Epoch: 17, Batch: 0, training loss: [0.6436473], training evaluation: {'auc': 0.650177, 'recall': 0.609375, 'precision': 0.61904764}\n",
      "Time: 480.93381094932556, Epoch: 17, Batch: 100, training loss: [0.67311466], training evaluation: {'auc': 0.6637057, 'recall': 0.5923077, 'precision': 0.62096775}\n",
      "Epoch: 17, validation loss: [0.67161775], validation evaluation: {'auc': 0.62189883, 'recall': 0.52558786, 'precision': 0.053791348}\n",
      "Time: 495.44059109687805, Epoch: 18, Batch: 0, training loss: [0.64395416], training evaluation: {'auc': 0.6946082, 'recall': 0.5942029, 'precision': 0.71304345}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 506.766236782074, Epoch: 18, Batch: 100, training loss: [0.6817192], training evaluation: {'auc': 0.61473346, 'recall': 0.5620438, 'precision': 0.6581197}\n",
      "Epoch: 18, validation loss: [0.69145375], validation evaluation: {'auc': 0.62234527, 'recall': 0.5578608, 'precision': 0.05250141}\n",
      "Time: 521.9950368404388, Epoch: 19, Batch: 0, training loss: [0.65253425], training evaluation: {'auc': 0.6545088, 'recall': 0.62903225, 'precision': 0.58208954}\n",
      "Time: 534.2088561058044, Epoch: 19, Batch: 100, training loss: [0.66510916], training evaluation: {'auc': 0.63361436, 'recall': 0.5645161, 'precision': 0.5882353}\n",
      "Epoch: 19, validation loss: [0.6243641], validation evaluation: {'auc': 0.6197378, 'recall': 0.4472107, 'precision': 0.056874815}\n"
     ]
    }
   ],
   "source": [
    "# If we want to reprodduce the result, the seed must be the same at the begining. Specifically in jupyter,\n",
    "# both of seed function and the main code should be rerun.\n",
    "np.random.seed(1000)\n",
    "rn.seed(30)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "epochs = 20\n",
    "learning_rate = 1e-4\n",
    "batch_size = 256\n",
    "\n",
    "embedding_dim = 50\n",
    "deep_hidden_units = [200, 100, 1]\n",
    "deep_drop_probs = [0.5, 0.5, 0.5]\n",
    "\n",
    "model = DeepFM(field_size, feat_size, embedding_dim, deep_hidden_units, deep_drop_probs)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)  \n",
    "model_wrapper = DeepFMWrapper(model)\n",
    "model_wrapper.fit(train, validation, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model_wrapper.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Saving and loading\n",
    "\n",
    "As mentioned in the official document of Tensorflow, the subclassed model cannot be saved in the way of the entire model, since it's the piece of code written in the call method. Instead, keras functional API see the model as a kind of data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_fn = '../models/deepFM/deepfm'\n",
    "model_wrapper.save_model(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = DeepFM(field_size, feat_size, embedding_dim, deep_hidden_units, deep_drop_probs)\n",
    "# new_model_wrapper = DeepFMWrapper(model)\n",
    "# new_model_wrapper.load_model(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_prediction = new_model_wrapper.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_prediction[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submission\n",
    "\n",
    "Kaggle Competition: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/leaderboard#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"../data/porto-seguro-safe-driver-prediction/sample_submission.csv\"\n",
    "submission = pd.read_csv(fn)\n",
    "submission['target'] = tf.sigmoid(test_prediction).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn = \"../data/porto-seguro-safe-driver-prediction/submission.csv\"\n",
    "submission.to_csv(out_fn, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
