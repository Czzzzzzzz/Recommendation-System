{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f6ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.7 (default, Sep 16 2021, 13:09:58) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "if os.path.join('..', '..', 'recommenders') not in sys.path:\n",
    "    sys.path.append(os.path.join('..', '..', 'recommenders'))\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import (\n",
    "    prepare_hparams\n",
    ")\n",
    "from recommenders.datasets.amazon_reviews import download_and_extract, data_preprocessing\n",
    "from recommenders.datasets.download_utils import maybe_download\n",
    "\n",
    "\n",
    "# Locally import the model\n",
    "from models.deeprec.models.sequential.din import DIN_RECModel as SeqModel\n",
    "\n",
    "\n",
    "#from recommenders.models.deeprec.models.sequential.nextitnet import NextItNetModel\n",
    "\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "#from recommenders.models.deeprec.io.nextitnet_iterator import NextItNetIterator\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "import pickle as pkl\n",
    "from recommenders.models.deeprec.deeprec_utils import load_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db6b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = '../../recommenders/models/deeprec/config/din.yaml'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "RANDOM_SEED = SEED  # Set None for non-deterministic result\n",
    "\n",
    "data_path = os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"slirec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002f6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for test\n",
    "train_file = os.path.join(data_path, r'train_data')\n",
    "valid_file = os.path.join(data_path, r'valid_data')\n",
    "test_file = os.path.join(data_path, r'test_data')\n",
    "user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
    "item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
    "output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "meta_name = 'meta_Movies_and_TV.json'\n",
    "reviews_file = os.path.join(data_path, reviews_name)\n",
    "meta_file = os.path.join(data_path, meta_name)\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "# sample_rate = 0.01 # sample a small item set for training and testing here for fast example\n",
    "sample_rate = 1\n",
    "\n",
    "input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    download_and_extract(reviews_name, reviews_file)\n",
    "    download_and_extract(meta_name, meta_file)\n",
    "    data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)\n",
    "    #### uncomment this for the NextItNet model, because it does not need to unfold the user history\n",
    "    # data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, is_history_expanding=False)\n",
    "\n",
    "# data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2301d",
   "metadata": {},
   "source": [
    "user_vocab: 123960\n",
    "\n",
    "item_vocab: 50024\n",
    "\n",
    "cate_vocab: 164\n",
    "\n",
    "hist_length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84667cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0., \n",
    "                          learning_rate=0.001,  # set to 0.01 if batch normalization is disable\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=1,\n",
    "                          MODEL_DIR=os.path.join(data_path, \"model/din_dice/\"),\n",
    "                          SUMMARIES_DIR=os.path.join(data_path, \"summary/din_dice/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "#                           attention_mode=\"inner_product\",\n",
    "#                           activation=['dice', 'dice'],\n",
    "#                           dice_momentum=0.9\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a804b2d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_entity': True,\n",
       " 'use_context': True,\n",
       " 'cross_activation': 'identity',\n",
       " 'user_dropout': True,\n",
       " 'dropout': [0.3, 0.3],\n",
       " 'attention_dropout': 0.0,\n",
       " 'load_saved_model': False,\n",
       " 'fast_CIN_d': 0,\n",
       " 'use_Linear_part': False,\n",
       " 'use_FM_part': False,\n",
       " 'use_CIN_part': False,\n",
       " 'use_DNN_part': False,\n",
       " 'init_method': 'tnormal',\n",
       " 'init_value': 0.01,\n",
       " 'embed_l2': 0.0,\n",
       " 'embed_l1': 0.0,\n",
       " 'layer_l2': 0.0,\n",
       " 'layer_l1': 0.0,\n",
       " 'cross_l2': 0.0,\n",
       " 'cross_l1': 0.0,\n",
       " 'reg_kg': 0.0,\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_rs': 1,\n",
       " 'lr_kg': 0.5,\n",
       " 'kg_training_interval': 5,\n",
       " 'max_grad_norm': 2,\n",
       " 'is_clip_norm': 0,\n",
       " 'dtype': 32,\n",
       " 'optimizer': 'adam',\n",
       " 'epochs': 10,\n",
       " 'batch_size': 400,\n",
       " 'enable_BN': True,\n",
       " 'show_step': 1,\n",
       " 'save_model': True,\n",
       " 'save_epoch': 1,\n",
       " 'write_tfevents': True,\n",
       " 'train_num_ngs': 4,\n",
       " 'need_sample': True,\n",
       " 'embedding_dropout': 0.0,\n",
       " 'EARLY_STOP': 10,\n",
       " 'min_seq_length': 1,\n",
       " 'slots': 5,\n",
       " 'cell': 'SUM',\n",
       " 'user_vocab': '../../tests/resources/deeprec/slirec/user_vocab.pkl',\n",
       " 'item_vocab': '../../tests/resources/deeprec/slirec/item_vocab.pkl',\n",
       " 'cate_vocab': '../../tests/resources/deeprec/slirec/category_vocab.pkl',\n",
       " 'method': 'classification',\n",
       " 'model_type': 'sli_rec',\n",
       " 'layer_sizes': [100, 64],\n",
       " 'att_fcn_layer_sizes': [80, 40],\n",
       " 'activation': ['relu', 'relu'],\n",
       " 'item_embedding_dim': 32,\n",
       " 'cate_embedding_dim': 8,\n",
       " 'user_embedding_dim': 16,\n",
       " 'attention_mode': 'inner_product',\n",
       " 'enable_softmax': True,\n",
       " 'dice_momentum': 0.99,\n",
       " 'use_time_to_now': False,\n",
       " 'loss': 'softmax',\n",
       " 'max_seq_length': 50,\n",
       " 'hidden_size': 40,\n",
       " 'attention_size': 40,\n",
       " 'metrics': ['auc', 'logloss'],\n",
       " 'pairwise_metrics': ['mean_mrr', 'ndcg@2;4;6', 'group_auc'],\n",
       " 'MODEL_DIR': '../../tests/resources/deeprec/slirec/model/din_dice/',\n",
       " 'SUMMARIES_DIR': '../../tests/resources/deeprec/slirec/summary/din_dice/'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "119d599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_creator = SequentialIterator\n",
    "#### uncomment this for the NextItNet model, because it needs a special data iterator for training\n",
    "#input_creator = NextItNetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6a1733",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caozheng/Desktop/code/recommenders/examples/00_quick_start/../../recommenders/models/deeprec/models/base_model.py:755: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
      "2022-06-18 08:31:27.187851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-18 08:31:27.193263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-18 08:31:27.193446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-18 08:31:27.744073: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-18 08:31:27.745225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-18 08:31:27.745401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-18 08:31:27.745521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-18 08:31:28.041512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-18 08:31:28.041663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-18 08:31:28.041772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-18 08:31:28.041863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9750 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2022-06-18 08:31:28.050810: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-06-18 08:33:16.576377: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 , total_loss: 1.6098, data_loss: 1.6098\n",
      "step 2 , total_loss: 1.6099, data_loss: 1.6099\n",
      "step 3 , total_loss: 1.6073, data_loss: 1.6073\n",
      "step 4 , total_loss: 1.6062, data_loss: 1.6062\n",
      "step 5 , total_loss: 1.6026, data_loss: 1.6026\n",
      "step 6 , total_loss: 1.6010, data_loss: 1.6010\n",
      "step 7 , total_loss: 1.5959, data_loss: 1.5959\n",
      "step 8 , total_loss: 1.5902, data_loss: 1.5902\n",
      "step 9 , total_loss: 1.5876, data_loss: 1.5876\n",
      "step 10 , total_loss: 1.5801, data_loss: 1.5801\n",
      "step 11 , total_loss: 1.5822, data_loss: 1.5822\n",
      "step 12 , total_loss: 1.5693, data_loss: 1.5693\n",
      "step 13 , total_loss: 1.5524, data_loss: 1.5524\n",
      "step 14 , total_loss: 1.5701, data_loss: 1.5701\n",
      "step 15 , total_loss: 1.5560, data_loss: 1.5560\n",
      "step 16 , total_loss: 1.5555, data_loss: 1.5555\n",
      "step 17 , total_loss: 1.5353, data_loss: 1.5353\n",
      "step 18 , total_loss: 1.5507, data_loss: 1.5507\n",
      "step 19 , total_loss: 1.5180, data_loss: 1.5180\n",
      "step 20 , total_loss: 1.5440, data_loss: 1.5440\n",
      "step 21 , total_loss: 1.5171, data_loss: 1.5171\n",
      "step 22 , total_loss: 1.5530, data_loss: 1.5530\n",
      "step 23 , total_loss: 1.5192, data_loss: 1.5192\n",
      "step 24 , total_loss: 1.5487, data_loss: 1.5487\n",
      "step 25 , total_loss: 1.5121, data_loss: 1.5121\n",
      "step 26 , total_loss: 1.5158, data_loss: 1.5158\n",
      "step 27 , total_loss: 1.4810, data_loss: 1.4810\n",
      "step 28 , total_loss: 1.5312, data_loss: 1.5312\n",
      "step 29 , total_loss: 1.5148, data_loss: 1.5148\n",
      "step 30 , total_loss: 1.5073, data_loss: 1.5073\n",
      "step 31 , total_loss: 1.4769, data_loss: 1.4769\n",
      "step 32 , total_loss: 1.4795, data_loss: 1.4795\n",
      "step 33 , total_loss: 1.4711, data_loss: 1.4711\n",
      "step 34 , total_loss: 1.4417, data_loss: 1.4417\n",
      "step 35 , total_loss: 1.4497, data_loss: 1.4497\n",
      "step 36 , total_loss: 1.4378, data_loss: 1.4378\n",
      "step 37 , total_loss: 1.4702, data_loss: 1.4702\n",
      "step 38 , total_loss: 1.4106, data_loss: 1.4106\n",
      "step 39 , total_loss: 1.3805, data_loss: 1.3805\n",
      "step 40 , total_loss: 1.4591, data_loss: 1.4591\n",
      "step 41 , total_loss: 1.4266, data_loss: 1.4266\n",
      "step 42 , total_loss: 1.3593, data_loss: 1.3593\n",
      "step 43 , total_loss: 1.3995, data_loss: 1.3995\n",
      "step 44 , total_loss: 1.3442, data_loss: 1.3442\n",
      "step 45 , total_loss: 1.3683, data_loss: 1.3683\n",
      "step 46 , total_loss: 1.3834, data_loss: 1.3834\n",
      "step 47 , total_loss: 1.3845, data_loss: 1.3845\n",
      "step 48 , total_loss: 1.3346, data_loss: 1.3346\n",
      "step 49 , total_loss: 1.3349, data_loss: 1.3349\n",
      "step 50 , total_loss: 1.3649, data_loss: 1.3649\n",
      "step 51 , total_loss: 1.3034, data_loss: 1.3034\n",
      "step 52 , total_loss: 1.2746, data_loss: 1.2746\n",
      "step 53 , total_loss: 1.2969, data_loss: 1.2969\n",
      "step 54 , total_loss: 1.3134, data_loss: 1.3134\n",
      "step 55 , total_loss: 1.2359, data_loss: 1.2359\n",
      "step 56 , total_loss: 1.3002, data_loss: 1.3002\n",
      "step 57 , total_loss: 1.2320, data_loss: 1.2320\n",
      "step 58 , total_loss: 1.2347, data_loss: 1.2347\n",
      "step 59 , total_loss: 1.2210, data_loss: 1.2210\n",
      "step 60 , total_loss: 1.2261, data_loss: 1.2261\n",
      "step 61 , total_loss: 1.1752, data_loss: 1.1752\n",
      "step 62 , total_loss: 1.1499, data_loss: 1.1499\n",
      "step 63 , total_loss: 1.1357, data_loss: 1.1357\n",
      "step 64 , total_loss: 1.1521, data_loss: 1.1521\n",
      "step 65 , total_loss: 1.1517, data_loss: 1.1517\n",
      "step 66 , total_loss: 1.1129, data_loss: 1.1129\n",
      "step 67 , total_loss: 1.0679, data_loss: 1.0679\n",
      "step 68 , total_loss: 1.0842, data_loss: 1.0842\n",
      "step 69 , total_loss: 1.0524, data_loss: 1.0524\n",
      "step 70 , total_loss: 1.0625, data_loss: 1.0625\n",
      "step 71 , total_loss: 1.0420, data_loss: 1.0420\n",
      "step 72 , total_loss: 1.0308, data_loss: 1.0308\n",
      "step 73 , total_loss: 1.0396, data_loss: 1.0396\n",
      "step 74 , total_loss: 0.9971, data_loss: 0.9971\n",
      "step 75 , total_loss: 0.9811, data_loss: 0.9811\n",
      "step 76 , total_loss: 0.9585, data_loss: 0.9585\n",
      "step 77 , total_loss: 0.9735, data_loss: 0.9735\n",
      "step 78 , total_loss: 0.9437, data_loss: 0.9437\n",
      "step 79 , total_loss: 0.9374, data_loss: 0.9374\n",
      "step 80 , total_loss: 0.8893, data_loss: 0.8893\n",
      "step 81 , total_loss: 0.9013, data_loss: 0.9013\n",
      "step 82 , total_loss: 0.9053, data_loss: 0.9053\n",
      "step 83 , total_loss: 0.8819, data_loss: 0.8819\n",
      "step 84 , total_loss: 0.8440, data_loss: 0.8440\n",
      "step 85 , total_loss: 0.8350, data_loss: 0.8350\n",
      "step 86 , total_loss: 0.8419, data_loss: 0.8419\n",
      "step 87 , total_loss: 0.8290, data_loss: 0.8290\n",
      "step 88 , total_loss: 0.8059, data_loss: 0.8059\n",
      "step 89 , total_loss: 0.8129, data_loss: 0.8129\n",
      "step 90 , total_loss: 0.7648, data_loss: 0.7648\n",
      "step 91 , total_loss: 0.7767, data_loss: 0.7767\n",
      "step 92 , total_loss: 0.7728, data_loss: 0.7728\n",
      "step 93 , total_loss: 0.7490, data_loss: 0.7490\n",
      "step 94 , total_loss: 0.7636, data_loss: 0.7636\n",
      "step 95 , total_loss: 0.7464, data_loss: 0.7464\n",
      "step 96 , total_loss: 0.6952, data_loss: 0.6952\n",
      "step 97 , total_loss: 0.7039, data_loss: 0.7039\n",
      "step 98 , total_loss: 0.6863, data_loss: 0.6863\n",
      "step 99 , total_loss: 0.6781, data_loss: 0.6781\n",
      "step 100 , total_loss: 0.6259, data_loss: 0.6259\n",
      "step 101 , total_loss: 0.6619, data_loss: 0.6619\n",
      "step 102 , total_loss: 0.6615, data_loss: 0.6615\n",
      "step 103 , total_loss: 0.6460, data_loss: 0.6460\n",
      "step 104 , total_loss: 0.6217, data_loss: 0.6217\n",
      "step 105 , total_loss: 0.6403, data_loss: 0.6403\n",
      "step 106 , total_loss: 0.6078, data_loss: 0.6078\n",
      "step 107 , total_loss: 0.6160, data_loss: 0.6160\n",
      "step 108 , total_loss: 0.5944, data_loss: 0.5944\n",
      "step 109 , total_loss: 0.5776, data_loss: 0.5776\n",
      "step 110 , total_loss: 0.5884, data_loss: 0.5884\n",
      "step 111 , total_loss: 0.5649, data_loss: 0.5649\n",
      "step 112 , total_loss: 0.5559, data_loss: 0.5559\n",
      "step 113 , total_loss: 0.5688, data_loss: 0.5688\n",
      "step 114 , total_loss: 0.5442, data_loss: 0.5442\n",
      "step 115 , total_loss: 0.5190, data_loss: 0.5190\n",
      "step 116 , total_loss: 0.5431, data_loss: 0.5431\n",
      "step 117 , total_loss: 0.5003, data_loss: 0.5003\n",
      "step 118 , total_loss: 0.5490, data_loss: 0.5490\n",
      "step 119 , total_loss: 0.5220, data_loss: 0.5220\n",
      "step 120 , total_loss: 0.5318, data_loss: 0.5318\n",
      "step 121 , total_loss: 0.4978, data_loss: 0.4978\n",
      "step 122 , total_loss: 0.4864, data_loss: 0.4864\n",
      "step 123 , total_loss: 0.4636, data_loss: 0.4636\n",
      "step 124 , total_loss: 0.4946, data_loss: 0.4946\n",
      "step 125 , total_loss: 0.4563, data_loss: 0.4563\n",
      "step 126 , total_loss: 0.4798, data_loss: 0.4798\n",
      "step 127 , total_loss: 0.4608, data_loss: 0.4608\n",
      "step 128 , total_loss: 0.4542, data_loss: 0.4542\n",
      "step 129 , total_loss: 0.4402, data_loss: 0.4402\n",
      "step 130 , total_loss: 0.4295, data_loss: 0.4295\n",
      "step 131 , total_loss: 0.4091, data_loss: 0.4091\n",
      "step 132 , total_loss: 0.4414, data_loss: 0.4414\n",
      "step 133 , total_loss: 0.3937, data_loss: 0.3937\n",
      "step 134 , total_loss: 0.4246, data_loss: 0.4246\n",
      "step 135 , total_loss: 0.3890, data_loss: 0.3890\n",
      "step 136 , total_loss: 0.3850, data_loss: 0.3850\n",
      "step 137 , total_loss: 0.4203, data_loss: 0.4203\n",
      "step 138 , total_loss: 0.4081, data_loss: 0.4081\n",
      "step 139 , total_loss: 0.4005, data_loss: 0.4005\n",
      "step 140 , total_loss: 0.4003, data_loss: 0.4003\n",
      "step 141 , total_loss: 0.3269, data_loss: 0.3269\n",
      "step 142 , total_loss: 0.3852, data_loss: 0.3852\n",
      "step 143 , total_loss: 0.3961, data_loss: 0.3961\n",
      "step 144 , total_loss: 0.3778, data_loss: 0.3778\n",
      "step 145 , total_loss: 0.3415, data_loss: 0.3415\n",
      "step 146 , total_loss: 0.3605, data_loss: 0.3605\n",
      "step 147 , total_loss: 0.3324, data_loss: 0.3324\n",
      "step 148 , total_loss: 0.3667, data_loss: 0.3667\n",
      "step 149 , total_loss: 0.3268, data_loss: 0.3268\n",
      "step 150 , total_loss: 0.3433, data_loss: 0.3433\n",
      "step 151 , total_loss: 0.3278, data_loss: 0.3278\n",
      "step 152 , total_loss: 0.3237, data_loss: 0.3237\n",
      "step 153 , total_loss: 0.3200, data_loss: 0.3200\n",
      "step 154 , total_loss: 0.3229, data_loss: 0.3229\n",
      "step 155 , total_loss: 0.3151, data_loss: 0.3151\n",
      "step 156 , total_loss: 0.3059, data_loss: 0.3059\n",
      "step 157 , total_loss: 0.3417, data_loss: 0.3417\n",
      "step 158 , total_loss: 0.3198, data_loss: 0.3198\n",
      "step 159 , total_loss: 0.2972, data_loss: 0.2972\n",
      "step 160 , total_loss: 0.3175, data_loss: 0.3175\n",
      "step 161 , total_loss: 0.3043, data_loss: 0.3043\n",
      "step 162 , total_loss: 0.2889, data_loss: 0.2889\n",
      "step 163 , total_loss: 0.3044, data_loss: 0.3044\n",
      "step 164 , total_loss: 0.2872, data_loss: 0.2872\n",
      "step 165 , total_loss: 0.2688, data_loss: 0.2688\n",
      "step 166 , total_loss: 0.2878, data_loss: 0.2878\n",
      "step 167 , total_loss: 0.2884, data_loss: 0.2884\n",
      "step 168 , total_loss: 0.2966, data_loss: 0.2966\n",
      "step 169 , total_loss: 0.2585, data_loss: 0.2585\n",
      "step 170 , total_loss: 0.2846, data_loss: 0.2846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 171 , total_loss: 0.2661, data_loss: 0.2661\n",
      "step 172 , total_loss: 0.2615, data_loss: 0.2615\n",
      "step 173 , total_loss: 0.2651, data_loss: 0.2651\n",
      "step 174 , total_loss: 0.2884, data_loss: 0.2884\n",
      "step 175 , total_loss: 0.2563, data_loss: 0.2563\n",
      "step 176 , total_loss: 0.2590, data_loss: 0.2590\n",
      "step 177 , total_loss: 0.2658, data_loss: 0.2658\n",
      "step 178 , total_loss: 0.2448, data_loss: 0.2448\n",
      "step 179 , total_loss: 0.2248, data_loss: 0.2248\n",
      "step 180 , total_loss: 0.2561, data_loss: 0.2561\n",
      "step 181 , total_loss: 0.2501, data_loss: 0.2501\n",
      "step 182 , total_loss: 0.3357, data_loss: 0.3357\n",
      "step 183 , total_loss: 0.2462, data_loss: 0.2462\n",
      "step 184 , total_loss: 0.2399, data_loss: 0.2399\n",
      "step 185 , total_loss: 0.2363, data_loss: 0.2363\n",
      "step 186 , total_loss: 0.2496, data_loss: 0.2496\n",
      "step 187 , total_loss: 0.2317, data_loss: 0.2317\n",
      "step 188 , total_loss: 0.2319, data_loss: 0.2319\n",
      "step 189 , total_loss: 0.2008, data_loss: 0.2008\n",
      "step 190 , total_loss: 0.2040, data_loss: 0.2040\n",
      "step 191 , total_loss: 0.2001, data_loss: 0.2001\n",
      "step 192 , total_loss: 0.2084, data_loss: 0.2084\n",
      "step 193 , total_loss: 0.2266, data_loss: 0.2266\n",
      "step 194 , total_loss: 0.3056, data_loss: 0.3056\n",
      "step 195 , total_loss: 0.1882, data_loss: 0.1882\n",
      "step 196 , total_loss: 0.2215, data_loss: 0.2215\n",
      "step 197 , total_loss: 0.1858, data_loss: 0.1858\n",
      "step 198 , total_loss: 0.2189, data_loss: 0.2189\n",
      "step 199 , total_loss: 0.1956, data_loss: 0.1956\n",
      "step 200 , total_loss: 0.2010, data_loss: 0.2010\n",
      "step 201 , total_loss: 0.1918, data_loss: 0.1918\n",
      "step 202 , total_loss: 0.1795, data_loss: 0.1795\n",
      "step 203 , total_loss: 0.1875, data_loss: 0.1875\n",
      "step 204 , total_loss: 0.1712, data_loss: 0.1712\n",
      "step 205 , total_loss: 0.1931, data_loss: 0.1931\n",
      "step 206 , total_loss: 0.1732, data_loss: 0.1732\n",
      "step 207 , total_loss: 0.1673, data_loss: 0.1673\n",
      "step 208 , total_loss: 0.1795, data_loss: 0.1795\n",
      "step 209 , total_loss: 0.1731, data_loss: 0.1731\n",
      "step 210 , total_loss: 0.1592, data_loss: 0.1592\n",
      "step 211 , total_loss: 0.1713, data_loss: 0.1713\n",
      "step 212 , total_loss: 0.1904, data_loss: 0.1904\n",
      "step 213 , total_loss: 0.2192, data_loss: 0.2192\n",
      "step 214 , total_loss: 0.1576, data_loss: 0.1576\n",
      "step 215 , total_loss: 0.2013, data_loss: 0.2013\n",
      "step 216 , total_loss: 0.1696, data_loss: 0.1696\n",
      "step 217 , total_loss: 0.1924, data_loss: 0.1924\n",
      "step 218 , total_loss: 0.1803, data_loss: 0.1803\n",
      "step 219 , total_loss: 0.1994, data_loss: 0.1994\n",
      "step 220 , total_loss: 0.1723, data_loss: 0.1723\n",
      "step 221 , total_loss: 0.1485, data_loss: 0.1485\n",
      "step 222 , total_loss: 0.1735, data_loss: 0.1735\n",
      "step 223 , total_loss: 0.1346, data_loss: 0.1346\n",
      "step 224 , total_loss: 0.1988, data_loss: 0.1988\n",
      "step 225 , total_loss: 0.1474, data_loss: 0.1474\n",
      "step 226 , total_loss: 0.1734, data_loss: 0.1734\n",
      "step 227 , total_loss: 0.1565, data_loss: 0.1565\n",
      "step 228 , total_loss: 0.1343, data_loss: 0.1343\n",
      "step 229 , total_loss: 0.2113, data_loss: 0.2113\n",
      "step 230 , total_loss: 0.1551, data_loss: 0.1551\n",
      "step 231 , total_loss: 0.2054, data_loss: 0.2054\n",
      "step 232 , total_loss: 0.1336, data_loss: 0.1336\n",
      "step 233 , total_loss: 0.1994, data_loss: 0.1994\n",
      "step 234 , total_loss: 0.1595, data_loss: 0.1595\n",
      "step 235 , total_loss: 0.1491, data_loss: 0.1491\n",
      "step 236 , total_loss: 0.1610, data_loss: 0.1610\n",
      "step 237 , total_loss: 0.1738, data_loss: 0.1738\n",
      "step 238 , total_loss: 0.1522, data_loss: 0.1522\n",
      "step 239 , total_loss: 0.1270, data_loss: 0.1270\n",
      "step 240 , total_loss: 0.1290, data_loss: 0.1290\n",
      "step 241 , total_loss: 0.1301, data_loss: 0.1301\n",
      "step 242 , total_loss: 0.1301, data_loss: 0.1301\n",
      "step 243 , total_loss: 0.1384, data_loss: 0.1384\n",
      "step 244 , total_loss: 0.1245, data_loss: 0.1245\n",
      "step 245 , total_loss: 0.1352, data_loss: 0.1352\n",
      "step 246 , total_loss: 0.1197, data_loss: 0.1197\n",
      "step 247 , total_loss: 0.1179, data_loss: 0.1179\n",
      "step 248 , total_loss: 0.1391, data_loss: 0.1391\n",
      "step 249 , total_loss: 0.1461, data_loss: 0.1461\n",
      "step 250 , total_loss: 0.1149, data_loss: 0.1149\n",
      "step 251 , total_loss: 0.1203, data_loss: 0.1203\n",
      "step 252 , total_loss: 0.1332, data_loss: 0.1332\n",
      "step 253 , total_loss: 0.1121, data_loss: 0.1121\n",
      "step 254 , total_loss: 0.1298, data_loss: 0.1298\n",
      "step 255 , total_loss: 0.1047, data_loss: 0.1047\n",
      "step 256 , total_loss: 0.1530, data_loss: 0.1530\n",
      "step 257 , total_loss: 0.1209, data_loss: 0.1209\n",
      "step 258 , total_loss: 0.1379, data_loss: 0.1379\n",
      "step 259 , total_loss: 0.1176, data_loss: 0.1176\n",
      "step 260 , total_loss: 0.1427, data_loss: 0.1427\n",
      "step 261 , total_loss: 0.1233, data_loss: 0.1233\n",
      "step 262 , total_loss: 0.1161, data_loss: 0.1161\n",
      "step 263 , total_loss: 0.1214, data_loss: 0.1214\n",
      "step 264 , total_loss: 0.1324, data_loss: 0.1324\n",
      "step 265 , total_loss: 0.1144, data_loss: 0.1144\n",
      "step 266 , total_loss: 0.0993, data_loss: 0.0993\n",
      "step 267 , total_loss: 0.0960, data_loss: 0.0960\n",
      "step 268 , total_loss: 0.1027, data_loss: 0.1027\n",
      "step 269 , total_loss: 0.1195, data_loss: 0.1195\n",
      "step 270 , total_loss: 0.1094, data_loss: 0.1094\n",
      "step 271 , total_loss: 0.0941, data_loss: 0.0941\n",
      "step 272 , total_loss: 0.0934, data_loss: 0.0934\n",
      "step 273 , total_loss: 0.0951, data_loss: 0.0951\n",
      "step 274 , total_loss: 0.0884, data_loss: 0.0884\n",
      "step 275 , total_loss: 0.0945, data_loss: 0.0945\n",
      "step 276 , total_loss: 0.1008, data_loss: 0.1008\n",
      "step 277 , total_loss: 0.0924, data_loss: 0.0924\n",
      "step 278 , total_loss: 0.1004, data_loss: 0.1004\n",
      "step 279 , total_loss: 0.1098, data_loss: 0.1098\n",
      "step 280 , total_loss: 0.0957, data_loss: 0.0957\n",
      "step 281 , total_loss: 0.0982, data_loss: 0.0982\n",
      "step 282 , total_loss: 0.0858, data_loss: 0.0858\n",
      "step 283 , total_loss: 0.1370, data_loss: 0.1370\n",
      "step 284 , total_loss: 0.0863, data_loss: 0.0863\n",
      "step 285 , total_loss: 0.1193, data_loss: 0.1193\n",
      "step 286 , total_loss: 0.0932, data_loss: 0.0932\n",
      "step 287 , total_loss: 0.0870, data_loss: 0.0870\n",
      "step 288 , total_loss: 0.1857, data_loss: 0.1857\n",
      "step 289 , total_loss: 0.0872, data_loss: 0.0872\n",
      "step 290 , total_loss: 0.1824, data_loss: 0.1824\n",
      "step 291 , total_loss: 0.1050, data_loss: 0.1050\n",
      "step 292 , total_loss: 0.1182, data_loss: 0.1182\n",
      "step 293 , total_loss: 0.1094, data_loss: 0.1094\n",
      "step 294 , total_loss: 0.0876, data_loss: 0.0876\n",
      "step 295 , total_loss: 0.1058, data_loss: 0.1058\n",
      "step 296 , total_loss: 0.0928, data_loss: 0.0928\n",
      "step 297 , total_loss: 0.1116, data_loss: 0.1116\n",
      "step 298 , total_loss: 0.0997, data_loss: 0.0997\n",
      "step 299 , total_loss: 0.0815, data_loss: 0.0815\n",
      "step 300 , total_loss: 0.0842, data_loss: 0.0842\n",
      "step 301 , total_loss: 0.0734, data_loss: 0.0734\n",
      "step 302 , total_loss: 0.0811, data_loss: 0.0811\n",
      "step 303 , total_loss: 0.0813, data_loss: 0.0813\n",
      "step 304 , total_loss: 0.0869, data_loss: 0.0869\n",
      "step 305 , total_loss: 0.0831, data_loss: 0.0831\n",
      "step 306 , total_loss: 0.0772, data_loss: 0.0772\n",
      "step 307 , total_loss: 0.0712, data_loss: 0.0712\n",
      "step 308 , total_loss: 0.0961, data_loss: 0.0961\n",
      "step 309 , total_loss: 0.0740, data_loss: 0.0740\n",
      "step 310 , total_loss: 0.0880, data_loss: 0.0880\n",
      "step 311 , total_loss: 0.0895, data_loss: 0.0895\n",
      "step 312 , total_loss: 0.0723, data_loss: 0.0723\n",
      "step 313 , total_loss: 0.0925, data_loss: 0.0925\n",
      "step 314 , total_loss: 0.0834, data_loss: 0.0834\n",
      "step 315 , total_loss: 0.0795, data_loss: 0.0795\n",
      "step 316 , total_loss: 0.0792, data_loss: 0.0792\n",
      "step 317 , total_loss: 0.0786, data_loss: 0.0786\n",
      "step 318 , total_loss: 0.0869, data_loss: 0.0869\n",
      "step 319 , total_loss: 0.0572, data_loss: 0.0572\n",
      "step 320 , total_loss: 0.0635, data_loss: 0.0635\n",
      "step 321 , total_loss: 0.0745, data_loss: 0.0745\n",
      "step 322 , total_loss: 0.0755, data_loss: 0.0755\n",
      "step 323 , total_loss: 0.0674, data_loss: 0.0674\n",
      "step 324 , total_loss: 0.0712, data_loss: 0.0712\n",
      "step 325 , total_loss: 0.0722, data_loss: 0.0722\n",
      "step 326 , total_loss: 0.0719, data_loss: 0.0719\n",
      "step 327 , total_loss: 0.0723, data_loss: 0.0723\n",
      "step 328 , total_loss: 0.0812, data_loss: 0.0812\n",
      "step 329 , total_loss: 0.0660, data_loss: 0.0660\n",
      "step 330 , total_loss: 0.0835, data_loss: 0.0835\n",
      "step 331 , total_loss: 0.0604, data_loss: 0.0604\n",
      "step 332 , total_loss: 0.0940, data_loss: 0.0940\n",
      "step 333 , total_loss: 0.0555, data_loss: 0.0555\n",
      "step 334 , total_loss: 0.0652, data_loss: 0.0652\n",
      "step 335 , total_loss: 0.0651, data_loss: 0.0651\n",
      "step 336 , total_loss: 0.0672, data_loss: 0.0672\n",
      "step 337 , total_loss: 0.0701, data_loss: 0.0701\n",
      "step 338 , total_loss: 0.0674, data_loss: 0.0674\n",
      "step 339 , total_loss: 0.0651, data_loss: 0.0651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 340 , total_loss: 0.0723, data_loss: 0.0723\n",
      "step 341 , total_loss: 0.0681, data_loss: 0.0681\n",
      "step 342 , total_loss: 0.0587, data_loss: 0.0587\n",
      "step 343 , total_loss: 0.0705, data_loss: 0.0705\n",
      "step 344 , total_loss: 0.0578, data_loss: 0.0578\n",
      "step 345 , total_loss: 0.0634, data_loss: 0.0634\n",
      "step 346 , total_loss: 0.0699, data_loss: 0.0699\n",
      "step 347 , total_loss: 0.0666, data_loss: 0.0666\n",
      "step 348 , total_loss: 0.0667, data_loss: 0.0667\n",
      "step 349 , total_loss: 0.0626, data_loss: 0.0626\n",
      "step 350 , total_loss: 0.0586, data_loss: 0.0586\n",
      "step 351 , total_loss: 0.0639, data_loss: 0.0639\n",
      "step 352 , total_loss: 0.0550, data_loss: 0.0550\n",
      "step 353 , total_loss: 0.0616, data_loss: 0.0616\n",
      "step 354 , total_loss: 0.0629, data_loss: 0.0629\n",
      "step 355 , total_loss: 0.0522, data_loss: 0.0522\n",
      "step 356 , total_loss: 0.0616, data_loss: 0.0616\n",
      "step 357 , total_loss: 0.0610, data_loss: 0.0610\n",
      "step 358 , total_loss: 0.0565, data_loss: 0.0565\n",
      "step 359 , total_loss: 0.0571, data_loss: 0.0571\n",
      "step 360 , total_loss: 0.0568, data_loss: 0.0568\n",
      "step 361 , total_loss: 0.0620, data_loss: 0.0620\n",
      "step 362 , total_loss: 0.0559, data_loss: 0.0559\n",
      "step 363 , total_loss: 0.0563, data_loss: 0.0563\n",
      "step 364 , total_loss: 0.0593, data_loss: 0.0593\n",
      "step 365 , total_loss: 0.0576, data_loss: 0.0576\n",
      "step 366 , total_loss: 0.0511, data_loss: 0.0511\n",
      "step 367 , total_loss: 0.0825, data_loss: 0.0825\n",
      "step 368 , total_loss: 0.0526, data_loss: 0.0526\n",
      "step 369 , total_loss: 0.0776, data_loss: 0.0776\n",
      "step 370 , total_loss: 0.0536, data_loss: 0.0536\n",
      "step 371 , total_loss: 0.0623, data_loss: 0.0623\n",
      "step 372 , total_loss: 0.0570, data_loss: 0.0570\n",
      "step 373 , total_loss: 0.0539, data_loss: 0.0539\n",
      "step 374 , total_loss: 0.0595, data_loss: 0.0595\n",
      "step 375 , total_loss: 0.0673, data_loss: 0.0673\n",
      "step 376 , total_loss: 0.0599, data_loss: 0.0599\n",
      "step 377 , total_loss: 0.0515, data_loss: 0.0515\n",
      "step 378 , total_loss: 0.0696, data_loss: 0.0696\n",
      "step 379 , total_loss: 0.0452, data_loss: 0.0452\n",
      "step 380 , total_loss: 0.0500, data_loss: 0.0500\n",
      "step 381 , total_loss: 0.0534, data_loss: 0.0534\n",
      "step 382 , total_loss: 0.0542, data_loss: 0.0542\n",
      "step 383 , total_loss: 0.0481, data_loss: 0.0481\n",
      "step 384 , total_loss: 0.0503, data_loss: 0.0503\n",
      "step 385 , total_loss: 0.0480, data_loss: 0.0480\n",
      "step 386 , total_loss: 0.0528, data_loss: 0.0528\n",
      "step 387 , total_loss: 0.0493, data_loss: 0.0493\n",
      "step 388 , total_loss: 0.0572, data_loss: 0.0572\n",
      "step 389 , total_loss: 0.0425, data_loss: 0.0425\n",
      "step 390 , total_loss: 0.0457, data_loss: 0.0457\n",
      "step 391 , total_loss: 0.0464, data_loss: 0.0464\n",
      "step 392 , total_loss: 0.0462, data_loss: 0.0462\n",
      "step 393 , total_loss: 0.0538, data_loss: 0.0538\n",
      "step 394 , total_loss: 0.0398, data_loss: 0.0398\n",
      "step 395 , total_loss: 0.0531, data_loss: 0.0531\n",
      "step 396 , total_loss: 0.0621, data_loss: 0.0621\n",
      "step 397 , total_loss: 0.0473, data_loss: 0.0473\n",
      "step 398 , total_loss: 0.0476, data_loss: 0.0476\n",
      "step 399 , total_loss: 0.0553, data_loss: 0.0553\n",
      "step 400 , total_loss: 0.0415, data_loss: 0.0415\n",
      "step 401 , total_loss: 0.0576, data_loss: 0.0576\n",
      "step 402 , total_loss: 0.0627, data_loss: 0.0627\n",
      "step 403 , total_loss: 0.0504, data_loss: 0.0504\n",
      "step 404 , total_loss: 0.0539, data_loss: 0.0539\n",
      "step 405 , total_loss: 0.0639, data_loss: 0.0639\n",
      "step 406 , total_loss: 0.0494, data_loss: 0.0494\n",
      "step 407 , total_loss: 0.0593, data_loss: 0.0593\n",
      "step 408 , total_loss: 0.0522, data_loss: 0.0522\n",
      "step 409 , total_loss: 0.0509, data_loss: 0.0509\n",
      "step 410 , total_loss: 0.0596, data_loss: 0.0596\n",
      "step 411 , total_loss: 0.0460, data_loss: 0.0460\n",
      "step 412 , total_loss: 0.0510, data_loss: 0.0510\n",
      "step 413 , total_loss: 0.0489, data_loss: 0.0489\n",
      "step 414 , total_loss: 0.0431, data_loss: 0.0431\n",
      "step 415 , total_loss: 0.0545, data_loss: 0.0545\n",
      "step 416 , total_loss: 0.0387, data_loss: 0.0387\n",
      "step 417 , total_loss: 0.0440, data_loss: 0.0440\n",
      "step 418 , total_loss: 0.0430, data_loss: 0.0430\n",
      "step 419 , total_loss: 0.0402, data_loss: 0.0402\n",
      "step 420 , total_loss: 0.0400, data_loss: 0.0400\n",
      "step 421 , total_loss: 0.0416, data_loss: 0.0416\n",
      "step 422 , total_loss: 0.0388, data_loss: 0.0388\n",
      "step 423 , total_loss: 0.0415, data_loss: 0.0415\n",
      "step 424 , total_loss: 0.0424, data_loss: 0.0424\n",
      "step 425 , total_loss: 0.0358, data_loss: 0.0358\n",
      "step 426 , total_loss: 0.0397, data_loss: 0.0397\n",
      "step 427 , total_loss: 0.0478, data_loss: 0.0478\n",
      "step 428 , total_loss: 0.0414, data_loss: 0.0414\n",
      "step 429 , total_loss: 0.0422, data_loss: 0.0422\n",
      "step 430 , total_loss: 0.0397, data_loss: 0.0397\n",
      "step 431 , total_loss: 0.0382, data_loss: 0.0382\n",
      "step 432 , total_loss: 0.0374, data_loss: 0.0374\n",
      "step 433 , total_loss: 0.0506, data_loss: 0.0506\n",
      "step 434 , total_loss: 0.0397, data_loss: 0.0397\n",
      "step 435 , total_loss: 0.0369, data_loss: 0.0369\n",
      "step 436 , total_loss: 0.0445, data_loss: 0.0445\n",
      "step 437 , total_loss: 0.0415, data_loss: 0.0415\n",
      "step 438 , total_loss: 0.0464, data_loss: 0.0464\n",
      "step 439 , total_loss: 0.0344, data_loss: 0.0344\n",
      "step 440 , total_loss: 0.0366, data_loss: 0.0366\n",
      "step 441 , total_loss: 0.0418, data_loss: 0.0418\n",
      "step 442 , total_loss: 0.0427, data_loss: 0.0427\n",
      "step 443 , total_loss: 0.0447, data_loss: 0.0447\n",
      "step 444 , total_loss: 0.0344, data_loss: 0.0344\n",
      "step 445 , total_loss: 0.0377, data_loss: 0.0377\n",
      "step 446 , total_loss: 0.0353, data_loss: 0.0353\n",
      "step 447 , total_loss: 0.0364, data_loss: 0.0364\n",
      "step 448 , total_loss: 0.0317, data_loss: 0.0317\n",
      "step 449 , total_loss: 0.0328, data_loss: 0.0328\n",
      "step 450 , total_loss: 0.0352, data_loss: 0.0352\n",
      "step 451 , total_loss: 0.0370, data_loss: 0.0370\n",
      "step 452 , total_loss: 0.0355, data_loss: 0.0355\n",
      "step 453 , total_loss: 0.0373, data_loss: 0.0373\n",
      "step 454 , total_loss: 0.0427, data_loss: 0.0427\n",
      "step 455 , total_loss: 0.0379, data_loss: 0.0379\n",
      "step 456 , total_loss: 0.0374, data_loss: 0.0374\n",
      "step 457 , total_loss: 0.0382, data_loss: 0.0382\n",
      "step 458 , total_loss: 0.0314, data_loss: 0.0314\n",
      "step 459 , total_loss: 0.0360, data_loss: 0.0360\n",
      "step 460 , total_loss: 0.0317, data_loss: 0.0317\n",
      "step 461 , total_loss: 0.0337, data_loss: 0.0337\n",
      "step 462 , total_loss: 0.0327, data_loss: 0.0327\n",
      "step 463 , total_loss: 0.0362, data_loss: 0.0362\n",
      "step 464 , total_loss: 0.0438, data_loss: 0.0438\n",
      "step 465 , total_loss: 0.0305, data_loss: 0.0305\n",
      "step 466 , total_loss: 0.0322, data_loss: 0.0322\n",
      "step 467 , total_loss: 0.0321, data_loss: 0.0321\n",
      "step 468 , total_loss: 0.0355, data_loss: 0.0355\n",
      "step 469 , total_loss: 0.0294, data_loss: 0.0294\n",
      "step 470 , total_loss: 0.0332, data_loss: 0.0332\n",
      "step 471 , total_loss: 0.0356, data_loss: 0.0356\n",
      "step 472 , total_loss: 0.0313, data_loss: 0.0313\n",
      "step 473 , total_loss: 0.0355, data_loss: 0.0355\n",
      "step 474 , total_loss: 0.0384, data_loss: 0.0384\n",
      "step 475 , total_loss: 0.0327, data_loss: 0.0327\n",
      "step 476 , total_loss: 0.0327, data_loss: 0.0327\n",
      "step 477 , total_loss: 0.0321, data_loss: 0.0321\n",
      "step 478 , total_loss: 0.0390, data_loss: 0.0390\n",
      "step 479 , total_loss: 0.0298, data_loss: 0.0298\n",
      "step 480 , total_loss: 0.0361, data_loss: 0.0361\n",
      "step 481 , total_loss: 0.0327, data_loss: 0.0327\n",
      "step 482 , total_loss: 0.0328, data_loss: 0.0328\n",
      "step 483 , total_loss: 0.0328, data_loss: 0.0328\n",
      "step 484 , total_loss: 0.0301, data_loss: 0.0301\n",
      "step 485 , total_loss: 0.0310, data_loss: 0.0310\n",
      "step 486 , total_loss: 0.0288, data_loss: 0.0288\n",
      "step 487 , total_loss: 0.0334, data_loss: 0.0334\n",
      "step 488 , total_loss: 0.0297, data_loss: 0.0297\n",
      "step 489 , total_loss: 0.0262, data_loss: 0.0262\n",
      "step 490 , total_loss: 0.0337, data_loss: 0.0337\n",
      "step 491 , total_loss: 0.0316, data_loss: 0.0316\n",
      "step 492 , total_loss: 0.0300, data_loss: 0.0300\n",
      "step 493 , total_loss: 0.0345, data_loss: 0.0345\n",
      "step 494 , total_loss: 0.0299, data_loss: 0.0299\n",
      "step 495 , total_loss: 0.0283, data_loss: 0.0283\n",
      "step 496 , total_loss: 0.0266, data_loss: 0.0266\n",
      "step 497 , total_loss: 0.0281, data_loss: 0.0281\n",
      "step 498 , total_loss: 0.0314, data_loss: 0.0314\n",
      "step 499 , total_loss: 0.0335, data_loss: 0.0335\n",
      "step 500 , total_loss: 0.0285, data_loss: 0.0285\n",
      "step 501 , total_loss: 0.0287, data_loss: 0.0287\n",
      "step 502 , total_loss: 0.0259, data_loss: 0.0259\n",
      "step 503 , total_loss: 0.0282, data_loss: 0.0282\n",
      "step 504 , total_loss: 0.0268, data_loss: 0.0268\n",
      "step 505 , total_loss: 0.0291, data_loss: 0.0291\n",
      "step 506 , total_loss: 0.0257, data_loss: 0.0257\n",
      "step 507 , total_loss: 0.0270, data_loss: 0.0270\n",
      "step 508 , total_loss: 0.0251, data_loss: 0.0251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 509 , total_loss: 0.0287, data_loss: 0.0287\n",
      "step 510 , total_loss: 0.0256, data_loss: 0.0256\n",
      "step 511 , total_loss: 0.0275, data_loss: 0.0275\n",
      "step 512 , total_loss: 0.0263, data_loss: 0.0263\n",
      "step 513 , total_loss: 0.0333, data_loss: 0.0333\n",
      "step 514 , total_loss: 0.0323, data_loss: 0.0323\n",
      "step 515 , total_loss: 0.0302, data_loss: 0.0302\n",
      "step 516 , total_loss: 0.0254, data_loss: 0.0254\n",
      "step 517 , total_loss: 0.0325, data_loss: 0.0325\n",
      "step 518 , total_loss: 0.0316, data_loss: 0.0316\n",
      "step 519 , total_loss: 0.0276, data_loss: 0.0276\n",
      "step 520 , total_loss: 0.0276, data_loss: 0.0276\n",
      "step 521 , total_loss: 0.0313, data_loss: 0.0313\n",
      "step 522 , total_loss: 0.0246, data_loss: 0.0246\n",
      "step 523 , total_loss: 0.0279, data_loss: 0.0279\n",
      "step 524 , total_loss: 0.0325, data_loss: 0.0325\n",
      "step 525 , total_loss: 0.0279, data_loss: 0.0279\n",
      "step 526 , total_loss: 0.0314, data_loss: 0.0314\n",
      "step 527 , total_loss: 0.0291, data_loss: 0.0291\n",
      "step 528 , total_loss: 0.0272, data_loss: 0.0272\n",
      "step 529 , total_loss: 0.0334, data_loss: 0.0334\n",
      "step 530 , total_loss: 0.0244, data_loss: 0.0244\n",
      "step 531 , total_loss: 0.0213, data_loss: 0.0213\n",
      "step 532 , total_loss: 0.0213, data_loss: 0.0213\n",
      "step 533 , total_loss: 0.0251, data_loss: 0.0251\n",
      "step 534 , total_loss: 0.0274, data_loss: 0.0274\n",
      "step 535 , total_loss: 0.0237, data_loss: 0.0237\n",
      "step 536 , total_loss: 0.0284, data_loss: 0.0284\n",
      "step 537 , total_loss: 0.0208, data_loss: 0.0208\n",
      "step 538 , total_loss: 0.0237, data_loss: 0.0237\n",
      "step 539 , total_loss: 0.0232, data_loss: 0.0232\n",
      "step 540 , total_loss: 0.0231, data_loss: 0.0231\n",
      "step 541 , total_loss: 0.0248, data_loss: 0.0248\n",
      "step 542 , total_loss: 0.0297, data_loss: 0.0297\n",
      "step 543 , total_loss: 0.0224, data_loss: 0.0224\n",
      "step 544 , total_loss: 0.0263, data_loss: 0.0263\n",
      "step 545 , total_loss: 0.0257, data_loss: 0.0257\n",
      "step 546 , total_loss: 0.0221, data_loss: 0.0221\n",
      "step 547 , total_loss: 0.0247, data_loss: 0.0247\n",
      "step 548 , total_loss: 0.0328, data_loss: 0.0328\n",
      "step 549 , total_loss: 0.0241, data_loss: 0.0241\n",
      "step 550 , total_loss: 0.0252, data_loss: 0.0252\n",
      "step 551 , total_loss: 0.0235, data_loss: 0.0235\n",
      "step 552 , total_loss: 0.0225, data_loss: 0.0225\n",
      "step 553 , total_loss: 0.0298, data_loss: 0.0298\n",
      "step 554 , total_loss: 0.0249, data_loss: 0.0249\n",
      "step 555 , total_loss: 0.0261, data_loss: 0.0261\n",
      "step 556 , total_loss: 0.0228, data_loss: 0.0228\n",
      "step 557 , total_loss: 0.0260, data_loss: 0.0260\n",
      "step 558 , total_loss: 0.0242, data_loss: 0.0242\n",
      "step 559 , total_loss: 0.0215, data_loss: 0.0215\n",
      "step 560 , total_loss: 0.0220, data_loss: 0.0220\n",
      "step 561 , total_loss: 0.0282, data_loss: 0.0282\n",
      "step 562 , total_loss: 0.0213, data_loss: 0.0213\n",
      "step 563 , total_loss: 0.0216, data_loss: 0.0216\n",
      "step 564 , total_loss: 0.0271, data_loss: 0.0271\n",
      "step 565 , total_loss: 0.0240, data_loss: 0.0240\n",
      "step 566 , total_loss: 0.0204, data_loss: 0.0204\n",
      "step 567 , total_loss: 0.0192, data_loss: 0.0192\n",
      "step 568 , total_loss: 0.0182, data_loss: 0.0182\n",
      "step 569 , total_loss: 0.0214, data_loss: 0.0214\n",
      "step 570 , total_loss: 0.0223, data_loss: 0.0223\n",
      "step 571 , total_loss: 0.0218, data_loss: 0.0218\n",
      "step 572 , total_loss: 0.0216, data_loss: 0.0216\n",
      "step 573 , total_loss: 0.0199, data_loss: 0.0199\n",
      "step 574 , total_loss: 0.0189, data_loss: 0.0189\n",
      "step 575 , total_loss: 0.0202, data_loss: 0.0202\n",
      "step 576 , total_loss: 0.0197, data_loss: 0.0197\n",
      "step 577 , total_loss: 0.0186, data_loss: 0.0186\n",
      "step 578 , total_loss: 0.0195, data_loss: 0.0195\n",
      "step 579 , total_loss: 0.0173, data_loss: 0.0173\n",
      "step 580 , total_loss: 0.0213, data_loss: 0.0213\n",
      "step 581 , total_loss: 0.0195, data_loss: 0.0195\n",
      "step 582 , total_loss: 0.0194, data_loss: 0.0194\n",
      "step 583 , total_loss: 0.0202, data_loss: 0.0202\n",
      "step 584 , total_loss: 0.0174, data_loss: 0.0174\n",
      "step 585 , total_loss: 0.0179, data_loss: 0.0179\n",
      "step 586 , total_loss: 0.0320, data_loss: 0.0320\n",
      "step 587 , total_loss: 0.0215, data_loss: 0.0215\n",
      "step 588 , total_loss: 0.0227, data_loss: 0.0227\n",
      "step 589 , total_loss: 0.0205, data_loss: 0.0205\n",
      "step 590 , total_loss: 0.0172, data_loss: 0.0172\n",
      "step 591 , total_loss: 0.0204, data_loss: 0.0204\n",
      "step 592 , total_loss: 0.0256, data_loss: 0.0256\n",
      "step 593 , total_loss: 0.0225, data_loss: 0.0225\n",
      "step 594 , total_loss: 0.0415, data_loss: 0.0415\n",
      "step 595 , total_loss: 0.0178, data_loss: 0.0178\n",
      "step 596 , total_loss: 0.0460, data_loss: 0.0460\n",
      "step 597 , total_loss: 0.0186, data_loss: 0.0186\n",
      "step 598 , total_loss: 0.0264, data_loss: 0.0264\n",
      "step 599 , total_loss: 0.0333, data_loss: 0.0333\n",
      "step 600 , total_loss: 0.0205, data_loss: 0.0205\n",
      "step 601 , total_loss: 0.0214, data_loss: 0.0214\n",
      "step 602 , total_loss: 0.0323, data_loss: 0.0323\n",
      "step 603 , total_loss: 0.0178, data_loss: 0.0178\n",
      "step 604 , total_loss: 0.0271, data_loss: 0.0271\n",
      "step 605 , total_loss: 0.0294, data_loss: 0.0294\n",
      "step 606 , total_loss: 0.0214, data_loss: 0.0214\n",
      "step 607 , total_loss: 0.0221, data_loss: 0.0221\n",
      "step 608 , total_loss: 0.0278, data_loss: 0.0278\n",
      "step 609 , total_loss: 0.0191, data_loss: 0.0191\n",
      "step 610 , total_loss: 0.0244, data_loss: 0.0244\n",
      "step 611 , total_loss: 0.0253, data_loss: 0.0253\n",
      "step 612 , total_loss: 0.0189, data_loss: 0.0189\n",
      "step 613 , total_loss: 0.0171, data_loss: 0.0171\n",
      "step 614 , total_loss: 0.0274, data_loss: 0.0274\n",
      "step 615 , total_loss: 0.0158, data_loss: 0.0158\n",
      "step 616 , total_loss: 0.0228, data_loss: 0.0228\n",
      "step 617 , total_loss: 0.0239, data_loss: 0.0239\n",
      "step 618 , total_loss: 0.0213, data_loss: 0.0213\n",
      "step 619 , total_loss: 0.0187, data_loss: 0.0187\n",
      "step 620 , total_loss: 0.0163, data_loss: 0.0163\n",
      "step 621 , total_loss: 0.0204, data_loss: 0.0204\n",
      "step 622 , total_loss: 0.0182, data_loss: 0.0182\n",
      "step 623 , total_loss: 0.0202, data_loss: 0.0202\n",
      "step 624 , total_loss: 0.0179, data_loss: 0.0179\n",
      "step 625 , total_loss: 0.0200, data_loss: 0.0200\n",
      "step 626 , total_loss: 0.0309, data_loss: 0.0309\n",
      "step 627 , total_loss: 0.0172, data_loss: 0.0172\n",
      "step 628 , total_loss: 0.0209, data_loss: 0.0209\n",
      "step 629 , total_loss: 0.0183, data_loss: 0.0183\n",
      "step 630 , total_loss: 0.0153, data_loss: 0.0153\n",
      "step 631 , total_loss: 0.0173, data_loss: 0.0173\n",
      "step 632 , total_loss: 0.0232, data_loss: 0.0232\n",
      "step 633 , total_loss: 0.0178, data_loss: 0.0178\n",
      "step 634 , total_loss: 0.0195, data_loss: 0.0195\n",
      "step 635 , total_loss: 0.0218, data_loss: 0.0218\n",
      "step 636 , total_loss: 0.0179, data_loss: 0.0179\n",
      "step 637 , total_loss: 0.0240, data_loss: 0.0240\n",
      "step 638 , total_loss: 0.0175, data_loss: 0.0175\n",
      "step 639 , total_loss: 0.0161, data_loss: 0.0161\n",
      "step 640 , total_loss: 0.0201, data_loss: 0.0201\n",
      "step 641 , total_loss: 0.0183, data_loss: 0.0183\n",
      "step 642 , total_loss: 0.0158, data_loss: 0.0158\n",
      "step 643 , total_loss: 0.0150, data_loss: 0.0150\n",
      "step 644 , total_loss: 0.0266, data_loss: 0.0266\n",
      "step 645 , total_loss: 0.0139, data_loss: 0.0139\n",
      "step 646 , total_loss: 0.0228, data_loss: 0.0228\n",
      "step 647 , total_loss: 0.0179, data_loss: 0.0179\n",
      "step 648 , total_loss: 0.0151, data_loss: 0.0151\n",
      "step 649 , total_loss: 0.0287, data_loss: 0.0287\n",
      "step 650 , total_loss: 0.0152, data_loss: 0.0152\n",
      "step 651 , total_loss: 0.0173, data_loss: 0.0173\n",
      "step 652 , total_loss: 0.0208, data_loss: 0.0208\n",
      "step 653 , total_loss: 0.0158, data_loss: 0.0158\n",
      "step 654 , total_loss: 0.0189, data_loss: 0.0189\n",
      "step 655 , total_loss: 0.0161, data_loss: 0.0161\n",
      "step 656 , total_loss: 0.0239, data_loss: 0.0239\n",
      "step 657 , total_loss: 0.0133, data_loss: 0.0133\n",
      "step 658 , total_loss: 0.0199, data_loss: 0.0199\n",
      "step 659 , total_loss: 0.0184, data_loss: 0.0184\n",
      "step 660 , total_loss: 0.0143, data_loss: 0.0143\n",
      "step 661 , total_loss: 0.0252, data_loss: 0.0252\n",
      "step 662 , total_loss: 0.0166, data_loss: 0.0166\n",
      "step 663 , total_loss: 0.0168, data_loss: 0.0168\n",
      "step 664 , total_loss: 0.0167, data_loss: 0.0167\n",
      "step 665 , total_loss: 0.0179, data_loss: 0.0179\n",
      "step 666 , total_loss: 0.0136, data_loss: 0.0136\n",
      "step 667 , total_loss: 0.0162, data_loss: 0.0162\n",
      "step 668 , total_loss: 0.0156, data_loss: 0.0156\n",
      "step 669 , total_loss: 0.0141, data_loss: 0.0141\n",
      "step 670 , total_loss: 0.0175, data_loss: 0.0175\n",
      "step 671 , total_loss: 0.0161, data_loss: 0.0161\n",
      "step 672 , total_loss: 0.0173, data_loss: 0.0173\n",
      "step 673 , total_loss: 0.0139, data_loss: 0.0139\n",
      "step 674 , total_loss: 0.0221, data_loss: 0.0221\n",
      "step 675 , total_loss: 0.0163, data_loss: 0.0163\n",
      "step 676 , total_loss: 0.0153, data_loss: 0.0153\n",
      "step 677 , total_loss: 0.0167, data_loss: 0.0167\n",
      "step 678 , total_loss: 0.0143, data_loss: 0.0143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 679 , total_loss: 0.0135, data_loss: 0.0135\n",
      "step 680 , total_loss: 0.0161, data_loss: 0.0161\n",
      "step 681 , total_loss: 0.0182, data_loss: 0.0182\n",
      "step 682 , total_loss: 0.0152, data_loss: 0.0152\n",
      "step 683 , total_loss: 0.0174, data_loss: 0.0174\n",
      "step 684 , total_loss: 0.0188, data_loss: 0.0188\n",
      "step 685 , total_loss: 0.0131, data_loss: 0.0131\n",
      "step 686 , total_loss: 0.0180, data_loss: 0.0180\n",
      "step 687 , total_loss: 0.0158, data_loss: 0.0158\n",
      "step 688 , total_loss: 0.0130, data_loss: 0.0130\n",
      "step 689 , total_loss: 0.0149, data_loss: 0.0149\n",
      "step 690 , total_loss: 0.0168, data_loss: 0.0168\n",
      "step 691 , total_loss: 0.0131, data_loss: 0.0131\n",
      "step 692 , total_loss: 0.0128, data_loss: 0.0128\n",
      "step 693 , total_loss: 0.0177, data_loss: 0.0177\n",
      "step 694 , total_loss: 0.0139, data_loss: 0.0139\n",
      "step 695 , total_loss: 0.0134, data_loss: 0.0134\n",
      "step 696 , total_loss: 0.0132, data_loss: 0.0132\n",
      "step 697 , total_loss: 0.0121, data_loss: 0.0121\n",
      "step 698 , total_loss: 0.0114, data_loss: 0.0114\n",
      "step 699 , total_loss: 0.0192, data_loss: 0.0192\n",
      "step 700 , total_loss: 0.0124, data_loss: 0.0124\n",
      "step 701 , total_loss: 0.0153, data_loss: 0.0153\n",
      "step 702 , total_loss: 0.0154, data_loss: 0.0154\n",
      "step 703 , total_loss: 0.0155, data_loss: 0.0155\n",
      "step 704 , total_loss: 0.0164, data_loss: 0.0164\n",
      "step 705 , total_loss: 0.0129, data_loss: 0.0129\n",
      "step 706 , total_loss: 0.0119, data_loss: 0.0119\n",
      "step 707 , total_loss: 0.0137, data_loss: 0.0137\n",
      "step 708 , total_loss: 0.0137, data_loss: 0.0137\n",
      "step 709 , total_loss: 0.0144, data_loss: 0.0144\n",
      "step 710 , total_loss: 0.0139, data_loss: 0.0139\n",
      "step 711 , total_loss: 0.0135, data_loss: 0.0135\n",
      "step 712 , total_loss: 0.0128, data_loss: 0.0128\n",
      "step 713 , total_loss: 0.0121, data_loss: 0.0121\n",
      "step 714 , total_loss: 0.0174, data_loss: 0.0174\n",
      "step 715 , total_loss: 0.0127, data_loss: 0.0127\n",
      "step 716 , total_loss: 0.0125, data_loss: 0.0125\n",
      "step 717 , total_loss: 0.0143, data_loss: 0.0143\n",
      "step 718 , total_loss: 0.0128, data_loss: 0.0128\n",
      "step 719 , total_loss: 0.0136, data_loss: 0.0136\n",
      "step 720 , total_loss: 0.0181, data_loss: 0.0181\n",
      "step 721 , total_loss: 0.0130, data_loss: 0.0130\n",
      "step 722 , total_loss: 0.0135, data_loss: 0.0135\n",
      "step 723 , total_loss: 0.0145, data_loss: 0.0145\n",
      "step 724 , total_loss: 0.0116, data_loss: 0.0116\n",
      "step 725 , total_loss: 0.0116, data_loss: 0.0116\n",
      "step 726 , total_loss: 0.0126, data_loss: 0.0126\n",
      "step 727 , total_loss: 0.0130, data_loss: 0.0130\n",
      "step 728 , total_loss: 0.0122, data_loss: 0.0122\n",
      "step 729 , total_loss: 0.0124, data_loss: 0.0124\n",
      "step 730 , total_loss: 0.0158, data_loss: 0.0158\n",
      "step 731 , total_loss: 0.0130, data_loss: 0.0130\n",
      "step 732 , total_loss: 0.0152, data_loss: 0.0152\n",
      "step 733 , total_loss: 0.0111, data_loss: 0.0111\n",
      "step 734 , total_loss: 0.0113, data_loss: 0.0113\n",
      "step 735 , total_loss: 0.0095, data_loss: 0.0095\n",
      "step 736 , total_loss: 0.0105, data_loss: 0.0105\n",
      "step 737 , total_loss: 0.0119, data_loss: 0.0119\n",
      "step 738 , total_loss: 0.0151, data_loss: 0.0151\n",
      "step 739 , total_loss: 0.0122, data_loss: 0.0122\n",
      "step 740 , total_loss: 0.0130, data_loss: 0.0130\n",
      "step 741 , total_loss: 0.0124, data_loss: 0.0124\n",
      "step 742 , total_loss: 0.0109, data_loss: 0.0109\n",
      "step 743 , total_loss: 0.0130, data_loss: 0.0130\n",
      "step 744 , total_loss: 0.0114, data_loss: 0.0114\n",
      "step 745 , total_loss: 0.0132, data_loss: 0.0132\n",
      "step 746 , total_loss: 0.0153, data_loss: 0.0153\n",
      "step 747 , total_loss: 0.0150, data_loss: 0.0150\n",
      "step 748 , total_loss: 0.0114, data_loss: 0.0114\n",
      "step 749 , total_loss: 0.0122, data_loss: 0.0122\n",
      "step 750 , total_loss: 0.0150, data_loss: 0.0150\n",
      "step 751 , total_loss: 0.0113, data_loss: 0.0113\n",
      "step 752 , total_loss: 0.0115, data_loss: 0.0115\n",
      "step 753 , total_loss: 0.0127, data_loss: 0.0127\n",
      "step 754 , total_loss: 0.0127, data_loss: 0.0127\n",
      "step 755 , total_loss: 0.0119, data_loss: 0.0119\n",
      "step 756 , total_loss: 0.0103, data_loss: 0.0103\n",
      "step 757 , total_loss: 0.0101, data_loss: 0.0101\n",
      "step 758 , total_loss: 0.0114, data_loss: 0.0114\n",
      "step 759 , total_loss: 0.0102, data_loss: 0.0102\n",
      "step 760 , total_loss: 0.0143, data_loss: 0.0143\n",
      "step 761 , total_loss: 0.0164, data_loss: 0.0164\n",
      "step 762 , total_loss: 0.0134, data_loss: 0.0134\n",
      "step 763 , total_loss: 0.0100, data_loss: 0.0100\n",
      "step 764 , total_loss: 0.0110, data_loss: 0.0110\n",
      "step 765 , total_loss: 0.0109, data_loss: 0.0109\n",
      "step 766 , total_loss: 0.0106, data_loss: 0.0106\n",
      "step 767 , total_loss: 0.0099, data_loss: 0.0099\n",
      "step 768 , total_loss: 0.0121, data_loss: 0.0121\n",
      "step 769 , total_loss: 0.0110, data_loss: 0.0110\n",
      "step 770 , total_loss: 0.0106, data_loss: 0.0106\n",
      "step 771 , total_loss: 0.0115, data_loss: 0.0115\n",
      "step 772 , total_loss: 0.0103, data_loss: 0.0103\n",
      "step 773 , total_loss: 0.0111, data_loss: 0.0111\n",
      "step 774 , total_loss: 0.0109, data_loss: 0.0109\n",
      "step 775 , total_loss: 0.0089, data_loss: 0.0089\n",
      "step 776 , total_loss: 0.0102, data_loss: 0.0102\n",
      "step 777 , total_loss: 0.0111, data_loss: 0.0111\n",
      "step 778 , total_loss: 0.0094, data_loss: 0.0094\n",
      "step 779 , total_loss: 0.0097, data_loss: 0.0097\n",
      "step 780 , total_loss: 0.0098, data_loss: 0.0098\n",
      "step 781 , total_loss: 0.0101, data_loss: 0.0101\n",
      "step 782 , total_loss: 0.0110, data_loss: 0.0110\n",
      "step 783 , total_loss: 0.0099, data_loss: 0.0099\n",
      "step 784 , total_loss: 0.0104, data_loss: 0.0104\n",
      "step 785 , total_loss: 0.0101, data_loss: 0.0101\n",
      "step 786 , total_loss: 0.0158, data_loss: 0.0158\n",
      "step 787 , total_loss: 0.0113, data_loss: 0.0113\n",
      "step 788 , total_loss: 0.0114, data_loss: 0.0114\n",
      "step 789 , total_loss: 0.0091, data_loss: 0.0091\n",
      "step 790 , total_loss: 0.0101, data_loss: 0.0101\n",
      "step 791 , total_loss: 0.0135, data_loss: 0.0135\n",
      "step 792 , total_loss: 0.0106, data_loss: 0.0106\n",
      "step 793 , total_loss: 0.0096, data_loss: 0.0096\n",
      "step 794 , total_loss: 0.0108, data_loss: 0.0108\n",
      "step 795 , total_loss: 0.0094, data_loss: 0.0094\n",
      "step 796 , total_loss: 0.0177, data_loss: 0.0177\n",
      "step 797 , total_loss: 0.0093, data_loss: 0.0093\n",
      "step 798 , total_loss: 0.0106, data_loss: 0.0106\n",
      "step 799 , total_loss: 0.0112, data_loss: 0.0112\n",
      "step 800 , total_loss: 0.0091, data_loss: 0.0091\n",
      "step 801 , total_loss: 0.0091, data_loss: 0.0091\n",
      "step 802 , total_loss: 0.0102, data_loss: 0.0102\n",
      "step 803 , total_loss: 0.0110, data_loss: 0.0110\n",
      "step 804 , total_loss: 0.0082, data_loss: 0.0082\n",
      "step 805 , total_loss: 0.0095, data_loss: 0.0095\n",
      "step 806 , total_loss: 0.0110, data_loss: 0.0110\n",
      "step 807 , total_loss: 0.0095, data_loss: 0.0095\n",
      "step 808 , total_loss: 0.0100, data_loss: 0.0100\n",
      "step 809 , total_loss: 0.0091, data_loss: 0.0091\n",
      "step 810 , total_loss: 0.0113, data_loss: 0.0113\n",
      "step 811 , total_loss: 0.0127, data_loss: 0.0127\n",
      "step 812 , total_loss: 0.0087, data_loss: 0.0087\n",
      "step 813 , total_loss: 0.0109, data_loss: 0.0109\n",
      "step 814 , total_loss: 0.0105, data_loss: 0.0105\n",
      "step 815 , total_loss: 0.0091, data_loss: 0.0091\n",
      "step 816 , total_loss: 0.0098, data_loss: 0.0098\n",
      "step 817 , total_loss: 0.0102, data_loss: 0.0102\n",
      "step 818 , total_loss: 0.0119, data_loss: 0.0119\n",
      "step 819 , total_loss: 0.0085, data_loss: 0.0085\n",
      "step 820 , total_loss: 0.0108, data_loss: 0.0108\n",
      "step 821 , total_loss: 0.0104, data_loss: 0.0104\n",
      "step 822 , total_loss: 0.0105, data_loss: 0.0105\n",
      "step 823 , total_loss: 0.0096, data_loss: 0.0096\n",
      "step 824 , total_loss: 0.0102, data_loss: 0.0102\n",
      "step 825 , total_loss: 0.0096, data_loss: 0.0096\n",
      "step 826 , total_loss: 0.0092, data_loss: 0.0092\n",
      "step 827 , total_loss: 0.0082, data_loss: 0.0082\n",
      "step 828 , total_loss: 0.0087, data_loss: 0.0087\n",
      "step 829 , total_loss: 0.0085, data_loss: 0.0085\n",
      "step 830 , total_loss: 0.0090, data_loss: 0.0090\n",
      "step 831 , total_loss: 0.0097, data_loss: 0.0097\n",
      "step 832 , total_loss: 0.0123, data_loss: 0.0123\n",
      "step 833 , total_loss: 0.0090, data_loss: 0.0090\n",
      "step 834 , total_loss: 0.0098, data_loss: 0.0098\n",
      "step 835 , total_loss: 0.0120, data_loss: 0.0120\n",
      "step 836 , total_loss: 0.0098, data_loss: 0.0098\n",
      "step 837 , total_loss: 0.0111, data_loss: 0.0111\n",
      "step 838 , total_loss: 0.0123, data_loss: 0.0123\n",
      "step 839 , total_loss: 0.0080, data_loss: 0.0080\n",
      "step 840 , total_loss: 0.0105, data_loss: 0.0105\n",
      "step 841 , total_loss: 0.0094, data_loss: 0.0094\n",
      "step 842 , total_loss: 0.0122, data_loss: 0.0122\n",
      "step 843 , total_loss: 0.0083, data_loss: 0.0083\n",
      "step 844 , total_loss: 0.0090, data_loss: 0.0090\n",
      "step 845 , total_loss: 0.0079, data_loss: 0.0079\n",
      "step 846 , total_loss: 0.0083, data_loss: 0.0083\n",
      "step 847 , total_loss: 0.0080, data_loss: 0.0080\n",
      "step 848 , total_loss: 0.0079, data_loss: 0.0079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 849 , total_loss: 0.0086, data_loss: 0.0086\n",
      "step 850 , total_loss: 0.0076, data_loss: 0.0076\n",
      "step 851 , total_loss: 0.0075, data_loss: 0.0075\n",
      "step 852 , total_loss: 0.0082, data_loss: 0.0082\n",
      "step 853 , total_loss: 0.0078, data_loss: 0.0078\n",
      "step 854 , total_loss: 0.0089, data_loss: 0.0089\n",
      "step 855 , total_loss: 0.0089, data_loss: 0.0089\n",
      "step 856 , total_loss: 0.0079, data_loss: 0.0079\n",
      "step 857 , total_loss: 0.0077, data_loss: 0.0077\n",
      "step 858 , total_loss: 0.0075, data_loss: 0.0075\n",
      "step 859 , total_loss: 0.0078, data_loss: 0.0078\n",
      "step 860 , total_loss: 0.0096, data_loss: 0.0096\n",
      "step 861 , total_loss: 0.0071, data_loss: 0.0071\n",
      "step 862 , total_loss: 0.0088, data_loss: 0.0088\n",
      "step 863 , total_loss: 0.0088, data_loss: 0.0088\n",
      "step 864 , total_loss: 0.0072, data_loss: 0.0072\n",
      "step 865 , total_loss: 0.0086, data_loss: 0.0086\n",
      "step 866 , total_loss: 0.0082, data_loss: 0.0082\n",
      "step 867 , total_loss: 0.0080, data_loss: 0.0080\n",
      "step 868 , total_loss: 0.0076, data_loss: 0.0076\n",
      "step 869 , total_loss: 0.0075, data_loss: 0.0075\n",
      "step 870 , total_loss: 0.0084, data_loss: 0.0084\n",
      "step 871 , total_loss: 0.0077, data_loss: 0.0077\n",
      "step 872 , total_loss: 0.0090, data_loss: 0.0090\n",
      "step 873 , total_loss: 0.0097, data_loss: 0.0097\n",
      "step 874 , total_loss: 0.0114, data_loss: 0.0114\n",
      "step 875 , total_loss: 0.0112, data_loss: 0.0112\n",
      "step 876 , total_loss: 0.0086, data_loss: 0.0086\n",
      "step 877 , total_loss: 0.0094, data_loss: 0.0094\n",
      "step 878 , total_loss: 0.0079, data_loss: 0.0079\n",
      "step 879 , total_loss: 0.0084, data_loss: 0.0084\n",
      "step 880 , total_loss: 0.0089, data_loss: 0.0089\n",
      "step 881 , total_loss: 0.0079, data_loss: 0.0079\n",
      "step 882 , total_loss: 0.0087, data_loss: 0.0087\n",
      "step 883 , total_loss: 0.0080, data_loss: 0.0080\n",
      "step 884 , total_loss: 0.0084, data_loss: 0.0084\n",
      "step 885 , total_loss: 0.0091, data_loss: 0.0091\n",
      "step 886 , total_loss: 0.0076, data_loss: 0.0076\n",
      "step 887 , total_loss: 0.0091, data_loss: 0.0091\n",
      "step 888 , total_loss: 0.0077, data_loss: 0.0077\n",
      "step 889 , total_loss: 0.0078, data_loss: 0.0078\n",
      "step 890 , total_loss: 0.0083, data_loss: 0.0083\n",
      "step 891 , total_loss: 0.0089, data_loss: 0.0089\n",
      "step 892 , total_loss: 0.0070, data_loss: 0.0070\n",
      "step 893 , total_loss: 0.0077, data_loss: 0.0077\n",
      "step 894 , total_loss: 0.0080, data_loss: 0.0080\n",
      "step 895 , total_loss: 0.0078, data_loss: 0.0078\n",
      "step 896 , total_loss: 0.0081, data_loss: 0.0081\n",
      "step 897 , total_loss: 0.0086, data_loss: 0.0086\n",
      "step 898 , total_loss: 0.0070, data_loss: 0.0070\n",
      "step 899 , total_loss: 0.0072, data_loss: 0.0072\n",
      "step 900 , total_loss: 0.0066, data_loss: 0.0066\n",
      "step 901 , total_loss: 0.0075, data_loss: 0.0075\n",
      "step 902 , total_loss: 0.0073, data_loss: 0.0073\n",
      "step 903 , total_loss: 0.0069, data_loss: 0.0069\n",
      "step 904 , total_loss: 0.0081, data_loss: 0.0081\n",
      "step 905 , total_loss: 0.0068, data_loss: 0.0068\n",
      "step 906 , total_loss: 0.0081, data_loss: 0.0081\n",
      "step 907 , total_loss: 0.0086, data_loss: 0.0086\n",
      "step 908 , total_loss: 0.0067, data_loss: 0.0067\n",
      "step 909 , total_loss: 0.0071, data_loss: 0.0071\n",
      "step 910 , total_loss: 0.0081, data_loss: 0.0081\n",
      "step 911 , total_loss: 0.0073, data_loss: 0.0073\n",
      "step 912 , total_loss: 0.0090, data_loss: 0.0090\n",
      "step 913 , total_loss: 0.0075, data_loss: 0.0075\n",
      "step 914 , total_loss: 0.0090, data_loss: 0.0090\n",
      "step 915 , total_loss: 0.0077, data_loss: 0.0077\n",
      "step 916 , total_loss: 0.0067, data_loss: 0.0067\n",
      "step 917 , total_loss: 0.0126, data_loss: 0.0126\n",
      "step 918 , total_loss: 0.0070, data_loss: 0.0070\n",
      "step 919 , total_loss: 0.0069, data_loss: 0.0069\n",
      "step 920 , total_loss: 0.0090, data_loss: 0.0090\n",
      "step 921 , total_loss: 0.0078, data_loss: 0.0078\n",
      "step 922 , total_loss: 0.0077, data_loss: 0.0077\n",
      "step 923 , total_loss: 0.0064, data_loss: 0.0064\n",
      "step 924 , total_loss: 0.0075, data_loss: 0.0075\n",
      "step 925 , total_loss: 0.0090, data_loss: 0.0090\n",
      "step 926 , total_loss: 0.0097, data_loss: 0.0097\n",
      "step 927 , total_loss: 0.0066, data_loss: 0.0066\n",
      "step 928 , total_loss: 0.0090, data_loss: 0.0090\n",
      "step 929 , total_loss: 0.0077, data_loss: 0.0077\n",
      "step 930 , total_loss: 0.0067, data_loss: 0.0067\n",
      "step 931 , total_loss: 0.0065, data_loss: 0.0065\n",
      "step 932 , total_loss: 0.0129, data_loss: 0.0129\n",
      "step 933 , total_loss: 0.0063, data_loss: 0.0063\n",
      "step 934 , total_loss: 0.0087, data_loss: 0.0087\n",
      "step 935 , total_loss: 0.0089, data_loss: 0.0089\n",
      "step 936 , total_loss: 0.0076, data_loss: 0.0076\n",
      "step 937 , total_loss: 0.0065, data_loss: 0.0065\n",
      "step 938 , total_loss: 0.0088, data_loss: 0.0088\n",
      "step 939 , total_loss: 0.0060, data_loss: 0.0060\n",
      "step 940 , total_loss: 0.0064, data_loss: 0.0064\n",
      "step 941 , total_loss: 0.0095, data_loss: 0.0095\n",
      "step 942 , total_loss: 0.0069, data_loss: 0.0069\n",
      "step 943 , total_loss: 0.0079, data_loss: 0.0079\n",
      "step 944 , total_loss: 0.0086, data_loss: 0.0086\n",
      "step 945 , total_loss: 0.0073, data_loss: 0.0073\n",
      "step 946 , total_loss: 0.0069, data_loss: 0.0069\n",
      "step 947 , total_loss: 0.0073, data_loss: 0.0073\n",
      "step 948 , total_loss: 0.0062, data_loss: 0.0062\n",
      "step 949 , total_loss: 0.0075, data_loss: 0.0075\n",
      "step 950 , total_loss: 0.0063, data_loss: 0.0063\n",
      "step 951 , total_loss: 0.0082, data_loss: 0.0082\n",
      "step 952 , total_loss: 0.0076, data_loss: 0.0076\n",
      "step 953 , total_loss: 0.0072, data_loss: 0.0072\n",
      "step 954 , total_loss: 0.0064, data_loss: 0.0064\n",
      "step 955 , total_loss: 0.0062, data_loss: 0.0062\n",
      "step 956 , total_loss: 0.0062, data_loss: 0.0062\n",
      "step 957 , total_loss: 0.0079, data_loss: 0.0079\n",
      "step 958 , total_loss: 0.0055, data_loss: 0.0055\n",
      "step 959 , total_loss: 0.0084, data_loss: 0.0084\n",
      "step 960 , total_loss: 0.0061, data_loss: 0.0061\n",
      "step 961 , total_loss: 0.0086, data_loss: 0.0086\n",
      "step 962 , total_loss: 0.0055, data_loss: 0.0055\n",
      "step 963 , total_loss: 0.0062, data_loss: 0.0062\n",
      "step 964 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 965 , total_loss: 0.0061, data_loss: 0.0061\n",
      "step 966 , total_loss: 0.0056, data_loss: 0.0056\n",
      "step 967 , total_loss: 0.0088, data_loss: 0.0088\n",
      "step 968 , total_loss: 0.0071, data_loss: 0.0071\n",
      "step 969 , total_loss: 0.0067, data_loss: 0.0067\n",
      "step 970 , total_loss: 0.0057, data_loss: 0.0057\n",
      "step 971 , total_loss: 0.0107, data_loss: 0.0107\n",
      "step 972 , total_loss: 0.0054, data_loss: 0.0054\n",
      "step 973 , total_loss: 0.0056, data_loss: 0.0056\n",
      "step 974 , total_loss: 0.0065, data_loss: 0.0065\n",
      "step 975 , total_loss: 0.0063, data_loss: 0.0063\n",
      "step 976 , total_loss: 0.0060, data_loss: 0.0060\n",
      "step 977 , total_loss: 0.0061, data_loss: 0.0061\n",
      "step 978 , total_loss: 0.0060, data_loss: 0.0060\n",
      "step 979 , total_loss: 0.0069, data_loss: 0.0069\n",
      "step 980 , total_loss: 0.0063, data_loss: 0.0063\n",
      "step 981 , total_loss: 0.0062, data_loss: 0.0062\n",
      "step 982 , total_loss: 0.0070, data_loss: 0.0070\n",
      "step 983 , total_loss: 0.0060, data_loss: 0.0060\n",
      "step 984 , total_loss: 0.0062, data_loss: 0.0062\n",
      "step 985 , total_loss: 0.0072, data_loss: 0.0072\n",
      "step 986 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 987 , total_loss: 0.0058, data_loss: 0.0058\n",
      "step 988 , total_loss: 0.0064, data_loss: 0.0064\n",
      "step 989 , total_loss: 0.0064, data_loss: 0.0064\n",
      "step 990 , total_loss: 0.0055, data_loss: 0.0055\n",
      "step 991 , total_loss: 0.0060, data_loss: 0.0060\n",
      "step 992 , total_loss: 0.0064, data_loss: 0.0064\n",
      "step 993 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 994 , total_loss: 0.0064, data_loss: 0.0064\n",
      "step 995 , total_loss: 0.0053, data_loss: 0.0053\n",
      "step 996 , total_loss: 0.0070, data_loss: 0.0070\n",
      "step 997 , total_loss: 0.0057, data_loss: 0.0057\n",
      "step 998 , total_loss: 0.0054, data_loss: 0.0054\n",
      "step 999 , total_loss: 0.0071, data_loss: 0.0071\n",
      "step 1000 , total_loss: 0.0066, data_loss: 0.0066\n",
      "step 1001 , total_loss: 0.0052, data_loss: 0.0052\n",
      "step 1002 , total_loss: 0.0055, data_loss: 0.0055\n",
      "step 1003 , total_loss: 0.0064, data_loss: 0.0064\n",
      "step 1004 , total_loss: 0.0067, data_loss: 0.0067\n",
      "step 1005 , total_loss: 0.0051, data_loss: 0.0051\n",
      "step 1006 , total_loss: 0.0055, data_loss: 0.0055\n",
      "step 1007 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 1008 , total_loss: 0.0058, data_loss: 0.0058\n",
      "step 1009 , total_loss: 0.0060, data_loss: 0.0060\n",
      "step 1010 , total_loss: 0.0058, data_loss: 0.0058\n",
      "step 1011 , total_loss: 0.0056, data_loss: 0.0056\n",
      "step 1012 , total_loss: 0.0053, data_loss: 0.0053\n",
      "step 1013 , total_loss: 0.0054, data_loss: 0.0054\n",
      "step 1014 , total_loss: 0.0060, data_loss: 0.0060\n",
      "step 1015 , total_loss: 0.0058, data_loss: 0.0058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1016 , total_loss: 0.0060, data_loss: 0.0060\n",
      "step 1017 , total_loss: 0.0057, data_loss: 0.0057\n",
      "step 1018 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1019 , total_loss: 0.0063, data_loss: 0.0063\n",
      "step 1020 , total_loss: 0.0054, data_loss: 0.0054\n",
      "step 1021 , total_loss: 0.0051, data_loss: 0.0051\n",
      "step 1022 , total_loss: 0.0049, data_loss: 0.0049\n",
      "step 1023 , total_loss: 0.0056, data_loss: 0.0056\n",
      "step 1024 , total_loss: 0.0055, data_loss: 0.0055\n",
      "step 1025 , total_loss: 0.0051, data_loss: 0.0051\n",
      "step 1026 , total_loss: 0.0055, data_loss: 0.0055\n",
      "step 1027 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 1028 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1029 , total_loss: 0.0065, data_loss: 0.0065\n",
      "step 1030 , total_loss: 0.0057, data_loss: 0.0057\n",
      "step 1031 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 1032 , total_loss: 0.0054, data_loss: 0.0054\n",
      "step 1033 , total_loss: 0.0050, data_loss: 0.0050\n",
      "step 1034 , total_loss: 0.0053, data_loss: 0.0053\n",
      "step 1035 , total_loss: 0.0074, data_loss: 0.0074\n",
      "step 1036 , total_loss: 0.0092, data_loss: 0.0092\n",
      "step 1037 , total_loss: 0.0055, data_loss: 0.0055\n",
      "step 1038 , total_loss: 0.0083, data_loss: 0.0083\n",
      "step 1039 , total_loss: 0.0102, data_loss: 0.0102\n",
      "step 1040 , total_loss: 0.0056, data_loss: 0.0056\n",
      "step 1041 , total_loss: 0.0092, data_loss: 0.0092\n",
      "step 1042 , total_loss: 0.0105, data_loss: 0.0105\n",
      "step 1043 , total_loss: 0.0053, data_loss: 0.0053\n",
      "step 1044 , total_loss: 0.0071, data_loss: 0.0071\n",
      "step 1045 , total_loss: 0.0105, data_loss: 0.0105\n",
      "step 1046 , total_loss: 0.0098, data_loss: 0.0098\n",
      "step 1047 , total_loss: 0.0068, data_loss: 0.0068\n",
      "step 1048 , total_loss: 0.0078, data_loss: 0.0078\n",
      "step 1049 , total_loss: 0.0076, data_loss: 0.0076\n",
      "step 1050 , total_loss: 0.0070, data_loss: 0.0070\n",
      "step 1051 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 1052 , total_loss: 0.0102, data_loss: 0.0102\n",
      "step 1053 , total_loss: 0.0076, data_loss: 0.0076\n",
      "step 1054 , total_loss: 0.0053, data_loss: 0.0053\n",
      "step 1055 , total_loss: 0.0083, data_loss: 0.0083\n",
      "step 1056 , total_loss: 0.0084, data_loss: 0.0084\n",
      "step 1057 , total_loss: 0.0110, data_loss: 0.0110\n",
      "step 1058 , total_loss: 0.0086, data_loss: 0.0086\n",
      "step 1059 , total_loss: 0.0078, data_loss: 0.0078\n",
      "step 1060 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 1061 , total_loss: 0.0057, data_loss: 0.0057\n",
      "step 1062 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 1063 , total_loss: 0.0056, data_loss: 0.0056\n",
      "step 1064 , total_loss: 0.0069, data_loss: 0.0069\n",
      "step 1065 , total_loss: 0.0065, data_loss: 0.0065\n",
      "step 1066 , total_loss: 0.0056, data_loss: 0.0056\n",
      "step 1067 , total_loss: 0.0067, data_loss: 0.0067\n",
      "step 1068 , total_loss: 0.0057, data_loss: 0.0057\n",
      "step 1069 , total_loss: 0.0057, data_loss: 0.0057\n",
      "step 1070 , total_loss: 0.0077, data_loss: 0.0077\n",
      "step 1071 , total_loss: 0.0049, data_loss: 0.0049\n",
      "step 1072 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1073 , total_loss: 0.0049, data_loss: 0.0049\n",
      "step 1074 , total_loss: 0.0044, data_loss: 0.0044\n",
      "step 1075 , total_loss: 0.0051, data_loss: 0.0051\n",
      "step 1076 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1077 , total_loss: 0.0052, data_loss: 0.0052\n",
      "step 1078 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1079 , total_loss: 0.0053, data_loss: 0.0053\n",
      "step 1080 , total_loss: 0.0048, data_loss: 0.0048\n",
      "step 1081 , total_loss: 0.0074, data_loss: 0.0074\n",
      "step 1082 , total_loss: 0.0049, data_loss: 0.0049\n",
      "step 1083 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 1084 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1085 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1086 , total_loss: 0.0085, data_loss: 0.0085\n",
      "step 1087 , total_loss: 0.0044, data_loss: 0.0044\n",
      "step 1088 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1089 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 1090 , total_loss: 0.0049, data_loss: 0.0049\n",
      "step 1091 , total_loss: 0.0054, data_loss: 0.0054\n",
      "step 1092 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1093 , total_loss: 0.0044, data_loss: 0.0044\n",
      "step 1094 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 1095 , total_loss: 0.0059, data_loss: 0.0059\n",
      "step 1096 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1097 , total_loss: 0.0048, data_loss: 0.0048\n",
      "step 1098 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1099 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1100 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1101 , total_loss: 0.0054, data_loss: 0.0054\n",
      "step 1102 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1103 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1104 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1105 , total_loss: 0.0049, data_loss: 0.0049\n",
      "step 1106 , total_loss: 0.0049, data_loss: 0.0049\n",
      "step 1107 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1108 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1109 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1110 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1111 , total_loss: 0.0053, data_loss: 0.0053\n",
      "step 1112 , total_loss: 0.0039, data_loss: 0.0039\n",
      "step 1113 , total_loss: 0.0038, data_loss: 0.0038\n",
      "step 1114 , total_loss: 0.0048, data_loss: 0.0048\n",
      "step 1115 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1116 , total_loss: 0.0051, data_loss: 0.0051\n",
      "step 1117 , total_loss: 0.0048, data_loss: 0.0048\n",
      "step 1118 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1119 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1120 , total_loss: 0.0042, data_loss: 0.0042\n",
      "step 1121 , total_loss: 0.0067, data_loss: 0.0067\n",
      "step 1122 , total_loss: 0.0054, data_loss: 0.0054\n",
      "step 1123 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1124 , total_loss: 0.0052, data_loss: 0.0052\n",
      "step 1125 , total_loss: 0.0052, data_loss: 0.0052\n",
      "step 1126 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1127 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1128 , total_loss: 0.0060, data_loss: 0.0060\n",
      "step 1129 , total_loss: 0.0039, data_loss: 0.0039\n",
      "step 1130 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1131 , total_loss: 0.0048, data_loss: 0.0048\n",
      "step 1132 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1133 , total_loss: 0.0034, data_loss: 0.0034\n",
      "step 1134 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1135 , total_loss: 0.0036, data_loss: 0.0036\n",
      "step 1136 , total_loss: 0.0042, data_loss: 0.0042\n",
      "step 1137 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1138 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1139 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1140 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1141 , total_loss: 0.0034, data_loss: 0.0034\n",
      "step 1142 , total_loss: 0.0060, data_loss: 0.0060\n",
      "step 1143 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1144 , total_loss: 0.0048, data_loss: 0.0048\n",
      "step 1145 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1146 , total_loss: 0.0038, data_loss: 0.0038\n",
      "step 1147 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1148 , total_loss: 0.0042, data_loss: 0.0042\n",
      "step 1149 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1150 , total_loss: 0.0053, data_loss: 0.0053\n",
      "step 1151 , total_loss: 0.0061, data_loss: 0.0061\n",
      "step 1152 , total_loss: 0.0050, data_loss: 0.0050\n",
      "step 1153 , total_loss: 0.0052, data_loss: 0.0052\n",
      "step 1154 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1155 , total_loss: 0.0044, data_loss: 0.0044\n",
      "step 1156 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1157 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1158 , total_loss: 0.0049, data_loss: 0.0049\n",
      "step 1159 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1160 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1161 , total_loss: 0.0053, data_loss: 0.0053\n",
      "step 1162 , total_loss: 0.0044, data_loss: 0.0044\n",
      "step 1163 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1164 , total_loss: 0.0044, data_loss: 0.0044\n",
      "step 1165 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1166 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1167 , total_loss: 0.0036, data_loss: 0.0036\n",
      "step 1168 , total_loss: 0.0039, data_loss: 0.0039\n",
      "step 1169 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1170 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1171 , total_loss: 0.0044, data_loss: 0.0044\n",
      "step 1172 , total_loss: 0.0042, data_loss: 0.0042\n",
      "step 1173 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1174 , total_loss: 0.0035, data_loss: 0.0035\n",
      "step 1175 , total_loss: 0.0035, data_loss: 0.0035\n",
      "step 1176 , total_loss: 0.0036, data_loss: 0.0036\n",
      "step 1177 , total_loss: 0.0055, data_loss: 0.0055\n",
      "step 1178 , total_loss: 0.0063, data_loss: 0.0063\n",
      "step 1179 , total_loss: 0.0047, data_loss: 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1180 , total_loss: 0.0051, data_loss: 0.0051\n",
      "step 1181 , total_loss: 0.0038, data_loss: 0.0038\n",
      "step 1182 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1183 , total_loss: 0.0037, data_loss: 0.0037\n",
      "step 1184 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1185 , total_loss: 0.0051, data_loss: 0.0051\n",
      "step 1186 , total_loss: 0.0038, data_loss: 0.0038\n",
      "step 1187 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1188 , total_loss: 0.0051, data_loss: 0.0051\n",
      "step 1189 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1190 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1191 , total_loss: 0.0036, data_loss: 0.0036\n",
      "step 1192 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1193 , total_loss: 0.0035, data_loss: 0.0035\n",
      "step 1194 , total_loss: 0.0052, data_loss: 0.0052\n",
      "step 1195 , total_loss: 0.0036, data_loss: 0.0036\n",
      "step 1196 , total_loss: 0.0038, data_loss: 0.0038\n",
      "step 1197 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1198 , total_loss: 0.0037, data_loss: 0.0037\n",
      "step 1199 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1200 , total_loss: 0.0038, data_loss: 0.0038\n",
      "step 1201 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1202 , total_loss: 0.0034, data_loss: 0.0034\n",
      "step 1203 , total_loss: 0.0039, data_loss: 0.0039\n",
      "step 1204 , total_loss: 0.0042, data_loss: 0.0042\n",
      "step 1205 , total_loss: 0.0034, data_loss: 0.0034\n",
      "step 1206 , total_loss: 0.0039, data_loss: 0.0039\n",
      "step 1207 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1208 , total_loss: 0.0037, data_loss: 0.0037\n",
      "step 1209 , total_loss: 0.0038, data_loss: 0.0038\n",
      "step 1210 , total_loss: 0.0036, data_loss: 0.0036\n",
      "step 1211 , total_loss: 0.0036, data_loss: 0.0036\n",
      "step 1212 , total_loss: 0.0039, data_loss: 0.0039\n",
      "step 1213 , total_loss: 0.0066, data_loss: 0.0066\n",
      "step 1214 , total_loss: 0.0037, data_loss: 0.0037\n",
      "step 1215 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1216 , total_loss: 0.0044, data_loss: 0.0044\n",
      "step 1217 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1218 , total_loss: 0.0037, data_loss: 0.0037\n",
      "step 1219 , total_loss: 0.0067, data_loss: 0.0067\n",
      "step 1220 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1221 , total_loss: 0.0037, data_loss: 0.0037\n",
      "step 1222 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1223 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1224 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1225 , total_loss: 0.0034, data_loss: 0.0034\n",
      "step 1226 , total_loss: 0.0034, data_loss: 0.0034\n",
      "step 1227 , total_loss: 0.0037, data_loss: 0.0037\n",
      "step 1228 , total_loss: 0.0042, data_loss: 0.0042\n",
      "step 1229 , total_loss: 0.0037, data_loss: 0.0037\n",
      "step 1230 , total_loss: 0.0033, data_loss: 0.0033\n",
      "step 1231 , total_loss: 0.0049, data_loss: 0.0049\n",
      "step 1232 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1233 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1234 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1235 , total_loss: 0.0039, data_loss: 0.0039\n",
      "step 1236 , total_loss: 0.0032, data_loss: 0.0032\n",
      "step 1237 , total_loss: 0.0034, data_loss: 0.0034\n",
      "step 1238 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1239 , total_loss: 0.0066, data_loss: 0.0066\n",
      "step 1240 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1241 , total_loss: 0.0049, data_loss: 0.0049\n",
      "step 1242 , total_loss: 0.0045, data_loss: 0.0045\n",
      "step 1243 , total_loss: 0.0035, data_loss: 0.0035\n",
      "step 1244 , total_loss: 0.0032, data_loss: 0.0032\n",
      "step 1245 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1246 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1247 , total_loss: 0.0032, data_loss: 0.0032\n",
      "step 1248 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1249 , total_loss: 0.0039, data_loss: 0.0039\n",
      "step 1250 , total_loss: 0.0044, data_loss: 0.0044\n",
      "step 1251 , total_loss: 0.0050, data_loss: 0.0050\n",
      "step 1252 , total_loss: 0.0039, data_loss: 0.0039\n",
      "step 1253 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1254 , total_loss: 0.0036, data_loss: 0.0036\n",
      "step 1255 , total_loss: 0.0032, data_loss: 0.0032\n",
      "step 1256 , total_loss: 0.0036, data_loss: 0.0036\n",
      "step 1257 , total_loss: 0.0030, data_loss: 0.0030\n",
      "step 1258 , total_loss: 0.0046, data_loss: 0.0046\n",
      "step 1259 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1260 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1261 , total_loss: 0.0033, data_loss: 0.0033\n",
      "step 1262 , total_loss: 0.0079, data_loss: 0.0079\n",
      "step 1263 , total_loss: 0.0048, data_loss: 0.0048\n",
      "step 1264 , total_loss: 0.0033, data_loss: 0.0033\n",
      "step 1265 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1266 , total_loss: 0.0037, data_loss: 0.0037\n",
      "step 1267 , total_loss: 0.0040, data_loss: 0.0040\n",
      "step 1268 , total_loss: 0.0035, data_loss: 0.0035\n",
      "step 1269 , total_loss: 0.0035, data_loss: 0.0035\n",
      "step 1270 , total_loss: 0.0056, data_loss: 0.0056\n",
      "step 1271 , total_loss: 0.0056, data_loss: 0.0056\n",
      "step 1272 , total_loss: 0.0036, data_loss: 0.0036\n",
      "step 1273 , total_loss: 0.0043, data_loss: 0.0043\n",
      "step 1274 , total_loss: 0.0057, data_loss: 0.0057\n",
      "step 1275 , total_loss: 0.0034, data_loss: 0.0034\n",
      "step 1276 , total_loss: 0.0041, data_loss: 0.0041\n",
      "step 1277 , total_loss: 0.0032, data_loss: 0.0032\n",
      "step 1278 , total_loss: 0.0033, data_loss: 0.0033\n",
      "step 1279 , total_loss: 0.0047, data_loss: 0.0047\n",
      "step 1280 , total_loss: 0.0038, data_loss: 0.0038\n",
      "step 1281 , total_loss: 0.0035, data_loss: 0.0035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1397683/2719419441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_num_ngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_num_ngs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# valid_num_ngs is the number of negative lines after each positive line in your valid_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/recommenders/examples/00_quick_start/../../recommenders/models/deeprec/models/sequential/sequential_base_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_file, valid_file, valid_num_ngs, eval_metric)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_data_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                     \u001b[0mstep_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_data_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_tfevents\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUMMARIES_DIR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/recommenders/examples/00_quick_start/../../recommenders/models/deeprec/models/base_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sess, feed_dict)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_keeps\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train_stage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         return sess.run(\n\u001b[0m\u001b[1;32m    433\u001b[0m             [\n\u001b[1;32m    434\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1371\u001b[0m                            run_metadata)\n\u001b[1;32m   1372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1361\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1451\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1452\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1453\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1454\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m                                             run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "# valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "# we will evaluate the performance of model on valid_file every epoch\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e41391",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_syn = model.run_eval(test_file, num_ngs=test_num_ngs)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c751c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best_trained = SeqModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "path_best_trained = os.path.join(hparams.MODEL_DIR, \"best_model\")\n",
    "print('loading saved model in {0}'.format(path_best_trained))\n",
    "model_best_trained.load_model(path_best_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3632c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dice + op\n",
    "{'auc': 0.8221,\n",
    " 'logloss': 1.0988,\n",
    " 'mean_mrr': 0.6528,\n",
    " 'ndcg@2': 0.5977,\n",
    " 'ndcg@4': 0.6752,\n",
    " 'ndcg@6': 0.708,\n",
    " 'group_auc': 0.8229}\n",
    " \n",
    "dice + op + disable bn\n",
    "{'auc': 0.8192,\n",
    " 'logloss': 1.1587,\n",
    " 'mean_mrr': 0.6455,\n",
    " 'ndcg@2': 0.5885,\n",
    " 'ndcg@4': 0.6685,\n",
    " 'ndcg@6': 0.7026, \n",
    " 'group_auc': 0.8199}\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "model_best_trained.run_eval(test_file, num_ngs=test_num_ngs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
